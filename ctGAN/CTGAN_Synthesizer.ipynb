{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()      # Upload the training data csv. The following implementation drops the null values. Custom handling before uploading is encouraged\n",
        "dataset_name = '...'      # use the name .csv file imported above\n",
        "attributes_to_ignore = []     # attributes with unique values such as names and ids that dont hold any information for the data and make learning harder\n",
        "discrete_columns = []"
      ],
      "metadata": {
        "id": "zaR-1PAVeeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(dataset_name)\n",
        "train_data = train_data.dropna()\n",
        "train_data = train_data.drop(attributes_to_ignore, axis = 1)\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "5mJPsRGKc_Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload the 'transformer.py' and 'data_sampler.py'\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "NPP0J4lzI5YQ",
        "outputId": "7a15201d-8f56-4310-d3aa-cba65d2fd259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da3a8851-286b-483d-8b5d-dfc9ae60070d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da3a8851-286b-483d-8b5d-dfc9ae60070d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_sampler.py to data_sampler.py\n",
            "Saving transformer.py to transformer.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_sampler.py': b'# -*- coding: utf-8 -*-\\n\"\"\"CTGAN_data_sampler.ipynb\\n\\nAutomatically generated by Colab.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1Nx032G5WjTNIKe6Nuly8E-31Xj3xzOWG\\n\"\"\"\\n\\n\"\"\"DataSampler module.\"\"\"\\n\\nimport numpy as np\\n\\n\\nclass DataSampler(object):\\n    \"\"\"DataSampler samples the conditional vector and corresponding data for CTGAN.\"\"\"\\n\\n    def __init__(self, data, output_info, log_frequency):\\n        self._data_length = len(data)\\n\\n        def is_discrete_column(column_info):\\n            return len(column_info) == 1 and column_info[0].activation_fn == \\'softmax\\'\\n\\n        n_discrete_columns = sum([\\n            1 for column_info in output_info if is_discrete_column(column_info)\\n        ])\\n\\n        self._discrete_column_matrix_st = np.zeros(n_discrete_columns, dtype=\\'int32\\')\\n\\n        # Store the row id for each category in each discrete column.\\n        # For example _rid_by_cat_cols[a][b] is a list of all rows with the\\n        # a-th discrete column equal value b.\\n        self._rid_by_cat_cols = []\\n\\n        # Compute _rid_by_cat_cols\\n        st = 0\\n        for column_info in output_info:\\n            if is_discrete_column(column_info):\\n                span_info = column_info[0]\\n                ed = st + span_info.dim  #span_info.dim is the number of categories of the discrete column\\n\\n                rid_by_cat = []\\n                for j in range(span_info.dim):  #For each category\\n                    rid_by_cat.append(np.nonzero(data[:, st + j])[0]) #np.nonzero returns the indices that correspond to nonzero values.\\n                self._rid_by_cat_cols.append(rid_by_cat) #So for every category a list of indices is created that matches the rows that belong in it. In the end we get a concatenation of these lists.\\n                #For each different column (meaning column_info in output_info as the latter contains info about all the columns) a rid_by_cat is created and the appended to _rid_by_cat_cols\\n                #So the ed result is a 2-D array where _rid_by_cat_cols[a][b] is a list of all rows with the a-th discrete column equal to value b\\n                st = ed\\n            else:\\n                st += sum([span_info.dim for span_info in column_info])\\n        assert st == data.shape[1]\\n\\n        # Prepare an interval matrix for efficiently sample conditional vector\\n        max_category = max(\\n            [column_info[0].dim for column_info in output_info if is_discrete_column(column_info)],\\n            default=0,\\n        )\\n\\n        self._discrete_column_cond_st = np.zeros(n_discrete_columns, dtype=\\'int32\\')\\n        self._discrete_column_n_category = np.zeros(n_discrete_columns, dtype=\\'int32\\')\\n        self._discrete_column_category_prob = np.zeros((n_discrete_columns, max_category))\\n        self._n_discrete_columns = n_discrete_columns\\n        self._n_categories = sum([\\n            column_info[0].dim for column_info in output_info if is_discrete_column(column_info)\\n        ])\\n\\n        st = 0\\n        current_id = 0\\n        current_cond_st = 0\\n        for column_info in output_info:\\n            if is_discrete_column(column_info):\\n                span_info = column_info[0]\\n                ed = st + span_info.dim\\n                category_freq = np.sum(data[:, st:ed], axis=0) #sums the 1 values found in each category of the current attribute\\n                if log_frequency:\\n                    category_freq = np.log(category_freq + 1)\\n                category_prob = category_freq / np.sum(category_freq) #normalizes the values tfrom 0 to 1\\n                self._discrete_column_category_prob[current_id, : span_info.dim] = category_prob\\n                self._discrete_column_cond_st[current_id] = current_cond_st\\n                self._discrete_column_n_category[current_id] = span_info.dim\\n                current_cond_st += span_info.dim\\n                current_id += 1\\n                st = ed\\n            else:\\n                st += sum([span_info.dim for span_info in column_info])\\n\\n    #sample a random category from the attribute \\'discrete_column_id\\'\\n    def _random_choice_prob_index(self, discrete_column_id):\\n        probs = self._discrete_column_category_prob[discrete_column_id] #that is an array with 1 row and span_info.dim columns\\n        r = np.expand_dims(np.random.rand(probs.shape[0]), axis=1) #returns basically one value with 2 dimensions to match probs\\n        return (probs.cumsum(axis=1) > r).argmax(axis=1) #cumsum is the cumulative sum. Every probability is added to the previous one and as we move forward, the first time the sum exceeds r, we select the corresponding column\\n\\n    def sample_condvec(self, batch):\\n        \"\"\"Generate the conditional vector for training.\\n\\n        Returns:\\n            cond (batch x #categories):\\n                The conditional vector.\\n            mask (batch x #discrete columns):\\n                A one-hot vector indicating the selected discrete column.\\n            discrete column id (batch):\\n                Integer representation of mask.\\n            category_id_in_col (batch):\\n                Selected category in the selected discrete column.\\n        \"\"\"\\n        if self._n_discrete_columns == 0:\\n            return None\\n\\n        discrete_column_id = np.random.choice(np.arange(self._n_discrete_columns), batch) #Samples randomly a total of \\'batch\\' column ids\\n\\n        cond = np.zeros((batch, self._n_categories), dtype=\\'float32\\')\\n        mask = np.zeros((batch, self._n_discrete_columns), dtype=\\'float32\\')\\n        mask[np.arange(batch), discrete_column_id] = 1 #For each row of the batch numbered for 0 to batch-1 it sets the corresponding column id to 1\\n        category_id_in_col = self._random_choice_prob_index(discrete_column_id)\\n        category_id = self._discrete_column_cond_st[discrete_column_id] + category_id_in_col\\n        cond[np.arange(batch), category_id] = 1\\n\\n        return cond, mask, discrete_column_id, category_id_in_col\\n        ## !!! Here mask vector seems to anly be dealing with the discrete columns and not also with the categories as described in the paper !!!\\n\\n    def sample_original_condvec(self, batch):\\n        \"\"\"Generate the conditional vector for generation use original frequency.\"\"\"\\n        if self._n_discrete_columns == 0:\\n            return None\\n\\n        category_freq = self._discrete_column_category_prob.flatten()\\n        category_freq = category_freq[category_freq != 0] # removes zero values\\n        category_freq = category_freq / np.sum(category_freq) #category_freq now contains the probability af each category of each colun all together normilized from 0 to 1\\n        col_idxs = np.random.choice(np.arange(len(category_freq)), batch, p=category_freq) #makes \\'batch\\' random choices based on category_freq distribution\\n        cond = np.zeros((batch, self._n_categories), dtype=\\'float32\\')\\n        cond[np.arange(batch), col_idxs] = 1\\n\\n        return cond\\n\\n    def sample_data(self, data, n, col, opt):\\n        \"\"\"Sample data from original training data satisfying the sampled conditional vector.\\n\\n        Args:\\n            data:\\n                The training data.\\n\\n        Returns:\\n            n:\\n                n rows of matrix data.\\n        \"\"\"\\n        ## col -> discrete_column_id\\n        ## opt -> category_id_in_col\\n        if col is None:\\n            idx = np.random.randint(len(data), size=n) #generates n random ints between 0 and len(data)\\n            return data[idx]\\n\\n        idx = []\\n        for c, o in zip(col, opt):\\n            idx.append(np.random.choice(self._rid_by_cat_cols[c][o])) #for all col,opt combinations given, return a row with the value opt in column col\\n\\n        return data[idx]\\n\\n    def dim_cond_vec(self):\\n        \"\"\"Return the total number of categories.\"\"\"\\n        return self._n_categories\\n\\n    def generate_cond_from_condition_column_info(self, condition_info, batch):\\n        \"\"\"Generate the condition vector.\"\"\"\\n        vec = np.zeros((batch, self._n_categories), dtype=\\'float32\\')\\n        id_ = self._discrete_column_matrix_st[condition_info[\\'discrete_column_id\\']] #condition_info comes from convert_column_name_value_to_id of the transformer module\\n        id_ += condition_info[\\'value_id\\']\\n        vec[:, id_] = 1\\n        return vec',\n",
              " 'transformer.py': b'# -*- coding: utf-8 -*-\\n\"\"\"CTGAN_Transformer.ipynb\\n\\nAutomatically generated by Colab.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1saeU9-5MYB9DK9JenugodWrN32_JFoMC\\n\"\"\"\\n\\n\"\"\"DataTransformer module.\"\"\"\\n\\nfrom collections import namedtuple\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom joblib import Parallel, delayed\\nfrom rdt.transformers import ClusterBasedNormalizer, OneHotEncoder\\n\\nSpanInfo = namedtuple(\\'SpanInfo\\', [\\'dim\\', \\'activation_fn\\'])\\nColumnTransformInfo = namedtuple(\\n    \\'ColumnTransformInfo\\',\\n    [\\'column_name\\', \\'column_type\\', \\'transform\\', \\'output_info\\', \\'output_dimensions\\'],\\n)\\n\\n\\nclass DataTransformer(object):\\n    \"\"\"Data Transformer.\\n\\n    Model continuous columns with a BayesianGMM and normalize them to a scalar between [-1, 1]\\n    and a vector. Discrete columns are encoded using a OneHotEncoder.\\n    \"\"\"\\n\\n    def __init__(self, max_clusters=10, weight_threshold=0.005):\\n        \"\"\"Create a data transformer.\\n\\n        Args:\\n            max_clusters (int):\\n                Maximum number of Gaussian distributions in Bayesian GMM.\\n            weight_threshold (float):\\n                Weight threshold for a Gaussian distribution to be kept.\\n        \"\"\"\\n        self._max_clusters = max_clusters\\n        self._weight_threshold = weight_threshold\\n\\n    def _fit_continuous(self, data):\\n        \"\"\"Train Bayesian GMM for continuous columns.\\n\\n        Args:\\n            data (pd.DataFrame):\\n                A dataframe containing a column.\\n\\n        Returns:\\n            namedtuple:\\n                A ``ColumnTransformInfo`` object.\\n        \"\"\"\\n        column_name = data.columns[0]\\n        gm = ClusterBasedNormalizer(\\n            missing_value_generation=\\'from_column\\',\\n            max_clusters=min(len(data), self._max_clusters),\\n            weight_threshold=self._weight_threshold,\\n        )\\n        gm.fit(data, column_name)\\n        num_components = sum(gm.valid_component_indicator)\\n\\n        return ColumnTransformInfo(\\n            column_name=column_name,\\n            column_type=\\'continuous\\',\\n            transform=gm, #the gm transformer is kept in this namedtuple for later use\\n            output_info=[SpanInfo(1, \\'tanh\\'), SpanInfo(num_components, \\'softmax\\')],\\n            output_dimensions=1 + num_components,\\n        )\\n\\n    def _fit_discrete(self, data):\\n        \"\"\"Fit one hot encoder for discrete column.\\n\\n        Args:\\n            data (pd.DataFrame):\\n                A dataframe containing a column.\\n\\n        Returns:\\n            namedtuple:\\n                A ``ColumnTransformInfo`` object.\\n        \"\"\"\\n        column_name = data.columns[0]\\n        ohe = OneHotEncoder()\\n        ohe.fit(data, column_name) #the onehotencoder is fit on the column\\n        num_categories = len(ohe.dummies)\\n\\n        return ColumnTransformInfo(\\n            column_name=column_name,\\n            column_type=\\'discrete\\',\\n            transform=ohe, #the onehotencoder is kept in this namedtuple for later use\\n            output_info=[SpanInfo(num_categories, \\'softmax\\')],\\n            output_dimensions=num_categories,\\n        )\\n\\n    def fit(self, raw_data, discrete_columns=()):\\n        \"\"\"Fit the ``DataTransformer``.\\n\\n        Fits a ``ClusterBasedNormalizer`` for continuous columns and a\\n        ``OneHotEncoder`` for discrete columns.\\n\\n        This step also counts the #columns in matrix data and span information.\\n        \"\"\"\\n        self.output_info_list = []\\n        self.output_dimensions = 0\\n        self.dataframe = True\\n\\n        if not isinstance(raw_data, pd.DataFrame):\\n            self.dataframe = False\\n            # work around for RDT issue #328 Fitting with numerical column names fails\\n            discrete_columns = [str(column) for column in discrete_columns]\\n            column_names = [str(num) for num in range(raw_data.shape[1])]\\n            raw_data = pd.DataFrame(raw_data, columns=column_names)\\n\\n        self._column_raw_dtypes = raw_data.infer_objects().dtypes\\n        self._column_transform_info_list = []\\n        for column_name in raw_data.columns:\\n            if column_name in discrete_columns:\\n                column_transform_info = self._fit_discrete(raw_data[[column_name]]) #_fit_discrete fits a OneHotEncoder in column \\'column_name\\'\\n            else:\\n                column_transform_info = self._fit_continuous(raw_data[[column_name]]) #_fit_continuous fits a GMM in column \\'column_name\\'\\n\\n            self.output_info_list.append(column_transform_info.output_info)\\n            self.output_dimensions += column_transform_info.output_dimensions\\n            self._column_transform_info_list.append(column_transform_info)\\n\\n    def _transform_continuous(self, column_transform_info, data): #Applies the Cluster Based Normilizer\\n        column_name = data.columns[0]\\n        flattened_column = data[column_name].to_numpy().flatten()\\n        data = data.assign(**{column_name: flattened_column})\\n        gm = column_transform_info.transform\\n        transformed = gm.transform(data)\\n\\n        #  Converts the transformed data to the appropriate output format.\\n        #  The first column (ending in \\'.normalized\\') stays the same,\\n        #  but the lable encoded column (ending in \\'.component\\') is one hot encoded.\\n        output = np.zeros((len(transformed), column_transform_info.output_dimensions))\\n        output[:, 0] = transformed[f\\'{column_name}.normalized\\'].to_numpy()\\n        index = transformed[f\\'{column_name}.component\\'].to_numpy().astype(int)\\n        output[np.arange(index.size), index + 1] = 1.0\\n\\n        return output\\n\\n    def _transform_discrete(self, column_transform_info, data): #Takes the OneHotEncoder from ColumnTransformInfo that was fit on the column and applies it to it\\n        ohe = column_transform_info.transform\\n        return ohe.transform(data).to_numpy()\\n\\n    def _synchronous_transform(self, raw_data, column_transform_info_list):\\n        \"\"\"Take a Pandas DataFrame and transform columns synchronous.\\n\\n        Outputs a list with Numpy arrays.\\n        \"\"\"\\n        column_data_list = []\\n        for column_transform_info in column_transform_info_list:\\n            column_name = column_transform_info.column_name\\n            data = raw_data[[column_name]]\\n            if column_transform_info.column_type == \\'continuous\\':\\n                column_data_list.append(self._transform_continuous(column_transform_info, data))\\n            else:\\n                column_data_list.append(self._transform_discrete(column_transform_info, data))\\n\\n        return column_data_list\\n\\n    def _parallel_transform(self, raw_data, column_transform_info_list):\\n        \"\"\"Take a Pandas DataFrame and transform columns in parallel.\\n\\n        Outputs a list with Numpy arrays.\\n        \"\"\"\\n        processes = []\\n        for column_transform_info in column_transform_info_list:\\n            column_name = column_transform_info.column_name\\n            data = raw_data[[column_name]]\\n            process = None\\n            if column_transform_info.column_type == \\'continuous\\':\\n                process = delayed(self._transform_continuous)(column_transform_info, data)\\n            else:\\n                process = delayed(self._transform_discrete)(column_transform_info, data)\\n            processes.append(process)\\n\\n        return Parallel(n_jobs=-1)(processes)\\n\\n    def transform(self, raw_data): #Here, _transform_discrete or _tranform_continuous is called through synchronous or parallel transform\\n        \"\"\"Take raw data and output a matrix data.\"\"\"\\n        if not isinstance(raw_data, pd.DataFrame):\\n            column_names = [str(num) for num in range(raw_data.shape[1])]\\n            raw_data = pd.DataFrame(raw_data, columns=column_names)\\n\\n        # Only use parallelization with larger data sizes.\\n        # Otherwise, the transformation will be slower.\\n        if raw_data.shape[0] < 500:\\n            column_data_list = self._synchronous_transform(\\n                raw_data, self._column_transform_info_list\\n            )\\n        else:\\n            column_data_list = self._parallel_transform(raw_data, self._column_transform_info_list)\\n\\n        return np.concatenate(column_data_list, axis=1).astype(float)\\n\\n    def _inverse_transform_continuous(self, column_transform_info, column_data, sigmas, st):\\n        gm = column_transform_info.transform\\n        data = pd.DataFrame(column_data[:, :2], columns=list(gm.get_output_sdtypes())).astype(float)\\n        data[data.columns[1]] = np.argmax(column_data[:, 1:], axis=1)\\n        if sigmas is not None:\\n            selected_normalized_value = np.random.normal(data.iloc[:, 0], sigmas[st])\\n            data.iloc[:, 0] = selected_normalized_value\\n\\n        return gm.reverse_transform(data)\\n\\n    def _inverse_transform_discrete(self, column_transform_info, column_data):\\n        ohe = column_transform_info.transform\\n        data = pd.DataFrame(column_data, columns=list(ohe.get_output_sdtypes()))\\n        return ohe.reverse_transform(data)[column_transform_info.column_name]\\n\\n    def inverse_transform(self, data, sigmas=None):\\n        \"\"\"Take matrix data and output raw data.\\n\\n        Output uses the same type as input to the transform function.\\n        Either np array or pd dataframe.\\n        \"\"\"\\n        st = 0\\n        recovered_column_data_list = []\\n        column_names = []\\n        for column_transform_info in self._column_transform_info_list:\\n            dim = column_transform_info.output_dimensions\\n            column_data = data[:, st : st + dim]\\n            if column_transform_info.column_type == \\'continuous\\':\\n                recovered_column_data = self._inverse_transform_continuous(\\n                    column_transform_info, column_data, sigmas, st\\n                )\\n            else:\\n                recovered_column_data = self._inverse_transform_discrete(\\n                    column_transform_info, column_data\\n                )\\n\\n            recovered_column_data_list.append(recovered_column_data)\\n            column_names.append(column_transform_info.column_name)\\n            st += dim\\n\\n        recovered_data = np.column_stack(recovered_column_data_list)\\n        recovered_data = pd.DataFrame(recovered_data, columns=column_names).astype(\\n            self._column_raw_dtypes\\n        )\\n        if not self.dataframe:\\n            recovered_data = recovered_data.to_numpy()\\n\\n        return recovered_data\\n\\n    def convert_column_name_value_to_id(self, column_name, value):\\n        \"\"\"Get the ids of the given `column_name`.\"\"\"\\n        discrete_counter = 0\\n        column_id = 0\\n        for column_transform_info in self._column_transform_info_list:\\n            if column_transform_info.column_name == column_name:\\n                break\\n            if column_transform_info.column_type == \\'discrete\\':\\n                discrete_counter += 1\\n\\n            column_id += 1\\n\\n        else:\\n            raise ValueError(f\"The column_name `{column_name}` doesn\\'t exist in the data.\")\\n\\n        ohe = column_transform_info.transform\\n        data = pd.DataFrame([value], columns=[column_transform_info.column_name])\\n        one_hot = ohe.transform(data).to_numpy()[0]\\n        if sum(one_hot) == 0:\\n            raise ValueError(f\"The value `{value}` doesn\\'t exist in the column `{column_name}`.\")\\n\\n        return {\\n            \\'discrete_column_id\\': discrete_counter,\\n            \\'column_id\\': column_id,\\n            \\'value_id\\': np.argmax(one_hot),\\n        }'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdt tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wHR7CsQgHZFn",
        "outputId": "c7368f9c-981f-4508-c0ac-34072fc917ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdt\n",
            "  Downloading rdt-1.17.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.11/dist-packages (from rdt) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from rdt) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from rdt) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from rdt) (1.6.1)\n",
            "Collecting Faker>=17 (from rdt)\n",
            "  Downloading faker-37.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.9 in /usr/local/lib/python3.11/dist-packages (from rdt) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from Faker>=17->rdt) (2025.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->rdt) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9->rdt) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt) (3.6.0)\n",
            "Downloading rdt-1.17.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.8/73.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-37.3.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Faker, rdt\n",
            "Successfully installed Faker-37.3.0 rdt-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVu0ew2bryx6"
      },
      "outputs": [],
      "source": [
        "# Implementation of the CTGAN synthesizer\n",
        "# GitHub repository: https://github.com/sdv-dev/CTGAN\n",
        "# Paper: https://arxiv.org/abs/1907.00503\n",
        "\n",
        "\n",
        "\"\"\"CTGAN module.\"\"\"\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional\n",
        "from tqdm import tqdm\n",
        "\n",
        "from data_sampler import DataSampler\n",
        "from transformer import DataTransformer\n",
        "#from ctgan.errors import InvalidDataError\n",
        "#from ctgan.synthesizers.base import BaseSynthesizer, random_state\n",
        "\n",
        "\n",
        "class Discriminator(Module):\n",
        "    \"\"\"Discriminator for the CTGAN.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, discriminator_dim, pac=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        dim = input_dim * pac\n",
        "        self.pac = pac\n",
        "        self.pacdim = dim\n",
        "        seq = []\n",
        "        for item in list(discriminator_dim):\n",
        "            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n",
        "            dim = item\n",
        "\n",
        "        seq += [Linear(dim, 1)]\n",
        "        self.seq = Sequential(*seq) # The * in *seq is used to unpack the list seq to create the sequential model\n",
        "\n",
        "    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
        "        \"\"\"Compute the gradient penalty.\"\"\"\n",
        "        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
        "        alpha = alpha.repeat(1, pac, real_data.size(1))\n",
        "        alpha = alpha.view(-1, real_data.size(1))   #These three lines created an array with repeating values between 0 and 1 for interpolation\n",
        "\n",
        "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "\n",
        "        disc_interpolates = self(interpolates)   #This returns the output of the discriminator with interpolates as input. Thats because this class comes from NNModule wcich calls \"forward\" automatically\n",
        "\n",
        "        gradients = torch.autograd.grad(\n",
        "            outputs=disc_interpolates,\n",
        "            inputs=interpolates,\n",
        "            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "            only_inputs=True,\n",
        "        )[0]\n",
        "\n",
        "        gradients_view = gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n",
        "        gradient_penalty = ((gradients_view) ** 2).mean() * lambda_\n",
        "\n",
        "        return gradient_penalty\n",
        "\n",
        "    def forward(self, input_):\n",
        "        \"\"\"Apply the Discriminator to the `input_`.\"\"\"\n",
        "        assert input_.size()[0] % self.pac == 0\n",
        "        return self.seq(input_.view(-1, self.pacdim)) #This ensures the forward passing through the torch.nn.Sequential model defined in _init_\n",
        "\n",
        "\n",
        "class Residual(Module):\n",
        "    \"\"\"Residual layer for the CTGAN.\"\"\"\n",
        "\n",
        "    def __init__(self, i, o):\n",
        "        super(Residual, self).__init__()\n",
        "        self.fc = Linear(i, o)\n",
        "        self.bn = BatchNorm1d(o)\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def forward(self, input_):\n",
        "        \"\"\"Apply the Residual layer to the `input_`.\"\"\"\n",
        "        out = self.fc(input_)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return torch.cat([out, input_], dim=1)\n",
        "\n",
        "\n",
        "class Generator(Module):\n",
        "    \"\"\"Generator for the CTGAN.\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, generator_dim, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        dim = embedding_dim\n",
        "        seq = []\n",
        "        for item in list(generator_dim):\n",
        "            seq += [Residual(dim, item)]\n",
        "            dim += item\n",
        "        seq.append(Linear(dim, data_dim))\n",
        "        self.seq = Sequential(*seq)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        \"\"\"Apply the Generator to the `input_`.\"\"\"\n",
        "        data = self.seq(input_)\n",
        "        return data\n",
        "\n",
        "\n",
        "class CTGAN(): #(BaseSynthesizer)\n",
        "    \"\"\"Conditional Table GAN Synthesizer.\n",
        "\n",
        "    This is the core class of the CTGAN project, where the different components\n",
        "    are orchestrated together.\n",
        "    For more details about the process, please check the [Modeling Tabular data using\n",
        "    Conditional GAN](https://arxiv.org/abs/1907.00503) paper.\n",
        "\n",
        "    Args:\n",
        "        embedding_dim (int):\n",
        "            Size of the random sample passed to the Generator. Defaults to 128.\n",
        "        generator_dim (tuple or list of ints):\n",
        "            Size of the output samples for each one of the Residuals. A Residual Layer\n",
        "            will be created for each one of the values provided. Defaults to (256, 256).\n",
        "        discriminator_dim (tuple or list of ints):\n",
        "            Size of the output samples for each one of the Discriminator Layers. A Linear Layer\n",
        "            will be created for each one of the values provided. Defaults to (256, 256).\n",
        "        generator_lr (float):\n",
        "            Learning rate for the generator. Defaults to 2e-4.\n",
        "        generator_decay (float):\n",
        "            Generator weight decay for the Adam Optimizer. Defaults to 1e-6.\n",
        "        discriminator_lr (float):\n",
        "            Learning rate for the discriminator. Defaults to 2e-4.\n",
        "        discriminator_decay (float):\n",
        "            Discriminator weight decay for the Adam Optimizer. Defaults to 1e-6.\n",
        "        batch_size (int):\n",
        "            Number of data samples to process in each step.\n",
        "        discriminator_steps (int):\n",
        "            Number of discriminator updates to do for each generator update.\n",
        "            From the WGAN paper: https://arxiv.org/abs/1701.07875. WGAN paper\n",
        "            default is 5. Default used is 1 to match original CTGAN implementation.\n",
        "        log_frequency (boolean):\n",
        "            Whether to use log frequency of categorical levels in conditional\n",
        "            sampling. Defaults to ``True``.\n",
        "        verbose (boolean):\n",
        "            Whether to have print statements for progress results. Defaults to ``False``.\n",
        "        epochs (int):\n",
        "            Number of training epochs. Defaults to 300.\n",
        "        pac (int):\n",
        "            Number of samples to group together when applying the discriminator.\n",
        "            Defaults to 10.\n",
        "        cuda (bool):\n",
        "            Whether to attempt to use cuda for GPU computation.\n",
        "            If this is False or CUDA is not available, CPU will be used.\n",
        "            Defaults to ``True``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim=128,\n",
        "        generator_dim=(256, 512, 256),\n",
        "        discriminator_dim=(256, 256),\n",
        "        generator_lr=2e-4,\n",
        "        generator_decay=1e-6,\n",
        "        discriminator_lr=2e-4,\n",
        "        discriminator_decay=1e-6,\n",
        "        batch_size=100,\n",
        "        discriminator_steps=1,\n",
        "        log_frequency=True,\n",
        "        verbose=False,\n",
        "        epochs=300,\n",
        "        pac=10,\n",
        "        cuda=True,\n",
        "    ):\n",
        "        assert batch_size % 2 == 0\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._generator_dim = generator_dim\n",
        "        self._discriminator_dim = discriminator_dim\n",
        "\n",
        "        self._generator_lr = generator_lr\n",
        "        self._generator_decay = generator_decay\n",
        "        self._discriminator_lr = discriminator_lr\n",
        "        self._discriminator_decay = discriminator_decay\n",
        "\n",
        "        self._batch_size = batch_size\n",
        "        self._discriminator_steps = discriminator_steps\n",
        "        self._log_frequency = log_frequency\n",
        "        self._verbose = verbose\n",
        "        self._epochs = epochs\n",
        "        self.pac = pac\n",
        "\n",
        "        if not cuda or not torch.cuda.is_available():\n",
        "            device = 'cpu'\n",
        "        elif isinstance(cuda, str):\n",
        "            device = cuda\n",
        "        else:\n",
        "            device = 'cuda'\n",
        "\n",
        "        self._device = torch.device(device)\n",
        "\n",
        "        self._transformer = None\n",
        "        self._data_sampler = None\n",
        "        self._generator = None\n",
        "        self.loss_values = None\n",
        "\n",
        "    @staticmethod\n",
        "    def _gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n",
        "        \"\"\"Deals with the instability of the gumbel_softmax for older versions of torch.\n",
        "\n",
        "        For more details about the issue:\n",
        "        https://drive.google.com/file/d/1AA5wPfZ1kquaRtVruCd6BiYZGcDeNxyP/view?usp=sharing\n",
        "\n",
        "        Args:\n",
        "            logits […, num_features]:\n",
        "                Unnormalized log probabilities\n",
        "            tau:\n",
        "                Non-negative scalar temperature\n",
        "            hard (bool):\n",
        "                If True, the returned samples will be discretized as one-hot vectors,\n",
        "                but will be differentiated as if it is the soft sample in autograd\n",
        "            dim (int):\n",
        "                A dimension along which softmax will be computed. Default: -1.\n",
        "\n",
        "        Returns:\n",
        "            Sampled tensor of same shape as logits from the Gumbel-Softmax distribution.\n",
        "        \"\"\"\n",
        "        for _ in range(10):\n",
        "            transformed = functional.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n",
        "            if not torch.isnan(transformed).any():\n",
        "                return transformed\n",
        "\n",
        "        raise ValueError('gumbel_softmax returning NaN.')\n",
        "\n",
        "    def _apply_activate(self, data):\n",
        "        \"\"\"Apply proper activation function to the output of the generator.\"\"\"\n",
        "        data_t = []\n",
        "        st = 0\n",
        "        for column_info in self._transformer.output_info_list:\n",
        "            for span_info in column_info:\n",
        "                if span_info.activation_fn == 'tanh':\n",
        "                    ed = st + span_info.dim\n",
        "                    data_t.append(torch.tanh(data[:, st:ed]))\n",
        "                    st = ed\n",
        "                elif span_info.activation_fn == 'softmax':\n",
        "                    ed = st + span_info.dim\n",
        "                    transformed = self._gumbel_softmax(data[:, st:ed], tau=0.2)\n",
        "                    data_t.append(transformed)\n",
        "                    st = ed\n",
        "                else:\n",
        "                    raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n",
        "\n",
        "        return torch.cat(data_t, dim=1)\n",
        "\n",
        "    def _cond_loss(self, data, c, m):\n",
        "        \"\"\"Compute the cross entropy loss on the fixed discrete column.\"\"\"\n",
        "        loss = []\n",
        "        st = 0\n",
        "        st_c = 0\n",
        "        for column_info in self._transformer.output_info_list:\n",
        "            for span_info in column_info:\n",
        "                if len(column_info) != 1 or span_info.activation_fn != 'softmax':\n",
        "                    # not discrete column\n",
        "                    st += span_info.dim\n",
        "                else:\n",
        "                    ed = st + span_info.dim\n",
        "                    ed_c = st_c + span_info.dim\n",
        "                    tmp = functional.cross_entropy(\n",
        "                        data[:, st:ed], torch.argmax(c[:, st_c:ed_c], dim=1), reduction='none'\n",
        "                    )\n",
        "                    loss.append(tmp)\n",
        "                    st = ed\n",
        "                    st_c = ed_c\n",
        "\n",
        "        loss = torch.stack(loss, dim=1)  # noqa: PD013\n",
        "\n",
        "        return (loss * m).sum() / data.size()[0]\n",
        "\n",
        "    def _validate_discrete_columns(self, train_data, discrete_columns):\n",
        "        \"\"\"Check whether ``discrete_columns`` exists in ``train_data``.\n",
        "\n",
        "        Args:\n",
        "            train_data (numpy.ndarray or pandas.DataFrame):\n",
        "                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n",
        "            discrete_columns (list-like):\n",
        "                List of discrete columns to be used to generate the Conditional\n",
        "                Vector. If ``train_data`` is a Numpy array, this list should\n",
        "                contain the integer indices of the columns. Otherwise, if it is\n",
        "                a ``pandas.DataFrame``, this list should contain the column names.\n",
        "        \"\"\"\n",
        "        if isinstance(train_data, pd.DataFrame):\n",
        "            invalid_columns = set(discrete_columns) - set(train_data.columns)\n",
        "        elif isinstance(train_data, np.ndarray):\n",
        "            invalid_columns = []\n",
        "            for column in discrete_columns:\n",
        "                if column < 0 or column >= train_data.shape[1]:\n",
        "                    invalid_columns.append(column)\n",
        "        else:\n",
        "            raise TypeError('``train_data`` should be either pd.DataFrame or np.array.')\n",
        "\n",
        "        if invalid_columns:\n",
        "            raise ValueError(f'Invalid columns found: {invalid_columns}')\n",
        "\n",
        "    def _validate_null_data(self, train_data, discrete_columns):\n",
        "        \"\"\"Check whether null values exist in continuous ``train_data``.\n",
        "\n",
        "        Args:\n",
        "            train_data (numpy.ndarray or pandas.DataFrame):\n",
        "                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n",
        "            discrete_columns (list-like):\n",
        "                List of discrete columns to be used to generate the Conditional\n",
        "                Vector. If ``train_data`` is a Numpy array, this list should\n",
        "                contain the integer indices of the columns. Otherwise, if it is\n",
        "                a ``pandas.DataFrame``, this list should contain the column names.\n",
        "        \"\"\"\n",
        "        if isinstance(train_data, pd.DataFrame):\n",
        "            continuous_cols = list(set(train_data.columns) - set(discrete_columns))\n",
        "            any_nulls = train_data[continuous_cols].isna().any().any()\n",
        "        else:\n",
        "            continuous_cols = [i for i in range(train_data.shape[1]) if i not in discrete_columns]\n",
        "            any_nulls = pd.DataFrame(train_data)[continuous_cols].isna().any().any()\n",
        "\n",
        "        if any_nulls:\n",
        "            raise ValueError(\n",
        "                'CTGAN does not support null values in the continuous training data. '\n",
        "                'Please remove all null values from your continuous training data.'\n",
        "            )\n",
        "\n",
        "    #random_state\n",
        "    def fit(self, train_data, discrete_columns=(), epochs=None):\n",
        "        \"\"\"Fit the CTGAN Synthesizer models to the training data.\n",
        "\n",
        "        Args:\n",
        "            train_data (numpy.ndarray or pandas.DataFrame):\n",
        "                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n",
        "            discrete_columns (list-like):\n",
        "                List of discrete columns to be used to generate the Conditional\n",
        "                Vector. If ``train_data`` is a Numpy array, this list should\n",
        "                contain the integer indices of the columns. Otherwise, if it is\n",
        "                a ``pandas.DataFrame``, this list should contain the column names.\n",
        "        \"\"\"\n",
        "        self._validate_discrete_columns(train_data, discrete_columns)\n",
        "        self._validate_null_data(train_data, discrete_columns)\n",
        "\n",
        "        if epochs is None:\n",
        "            epochs = self._epochs\n",
        "        else:\n",
        "            warnings.warn(\n",
        "                (\n",
        "                    '`epochs` argument in `fit` method has been deprecated and will be removed '\n",
        "                    'in a future version. Please pass `epochs` to the constructor instead'\n",
        "                ),\n",
        "                DeprecationWarning,\n",
        "            )\n",
        "\n",
        "        self._transformer = DataTransformer()\n",
        "        self._transformer.fit(train_data, discrete_columns)\n",
        "\n",
        "        train_data = self._transformer.transform(train_data)\n",
        "\n",
        "        self._data_sampler = DataSampler(\n",
        "            train_data, self._transformer.output_info_list, self._log_frequency\n",
        "        )\n",
        "\n",
        "        data_dim = self._transformer.output_dimensions\n",
        "\n",
        "        self._generator = Generator(\n",
        "            self._embedding_dim + self._data_sampler.dim_cond_vec(), self._generator_dim, data_dim\n",
        "        ).to(self._device)\n",
        "\n",
        "        discriminator = Discriminator(\n",
        "            data_dim + self._data_sampler.dim_cond_vec(), self._discriminator_dim, pac=self.pac\n",
        "        ).to(self._device)\n",
        "\n",
        "        optimizerG = optim.Adam(\n",
        "            self._generator.parameters(),\n",
        "            lr=self._generator_lr,\n",
        "            betas=(0.5, 0.9),\n",
        "            weight_decay=self._generator_decay,\n",
        "        )\n",
        "\n",
        "        optimizerD = optim.Adam(\n",
        "            discriminator.parameters(),\n",
        "            lr=self._discriminator_lr,\n",
        "            betas=(0.5, 0.9),\n",
        "            weight_decay=self._discriminator_decay,\n",
        "        )\n",
        "\n",
        "        mean = torch.zeros(self._batch_size, self._embedding_dim, device=self._device)\n",
        "        std = mean + 1\n",
        "\n",
        "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Generator Loss', 'Distriminator Loss'])\n",
        "\n",
        "        epoch_iterator = tqdm(range(epochs), disable=(not self._verbose))\n",
        "        if self._verbose:\n",
        "            description = 'Gen. ({gen:.2f}) | Discrim. ({dis:.2f})'\n",
        "            epoch_iterator.set_description(description.format(gen=0, dis=0))\n",
        "\n",
        "        steps_per_epoch = max(len(train_data) // self._batch_size, 1)\n",
        "        for i in epoch_iterator:\n",
        "            for id_ in range(steps_per_epoch):\n",
        "                for n in range(self._discriminator_steps):\n",
        "                    fakez = torch.normal(mean=mean, std=std) #gives a scalar sampled from normal distribution with mean=mean and std=std\n",
        "\n",
        "                    condvec = self._data_sampler.sample_condvec(self._batch_size) #returns _batch_size number of results of (cond vector, mask vector, ids of the discrete columns, category in each discrete column)\n",
        "                    if condvec is None:\n",
        "                        c1, m1, col, opt = None, None, None, None\n",
        "                        real = self._data_sampler.sample_data(\n",
        "                            train_data, self._batch_size, col, opt\n",
        "                        )\n",
        "                    else:\n",
        "                        c1, m1, col, opt = condvec\n",
        "                        c1 = torch.from_numpy(c1).to(self._device)\n",
        "                        m1 = torch.from_numpy(m1).to(self._device)\n",
        "                        fakez = torch.cat([fakez, c1], dim=1) #fakez is a tensor starting with a random float and then the cond vector c1\n",
        "\n",
        "                        perm = np.arange(self._batch_size)\n",
        "                        np.random.shuffle(perm)\n",
        "                        #sample_data samples batch_size rows from the real data. The rows' discrete columns match the cond vectors sampled in sample_condvec\n",
        "                        real = self._data_sampler.sample_data(\n",
        "                            train_data, self._batch_size, col[perm], opt[perm] #Col is an integer representation of mask and contains batch number of column ids, opt contains the corresponding chosen category of each column\n",
        "                        )\n",
        "                        c2 = c1[perm]\n",
        "\n",
        "                    fake = self._generator(fakez)\n",
        "                    fakeact = self._apply_activate(fake)\n",
        "\n",
        "                    real = torch.from_numpy(real.astype('float32')).to(self._device)\n",
        "\n",
        "                    if c1 is not None:\n",
        "                        fake_cat = torch.cat([fakeact, c1], dim=1)\n",
        "                        real_cat = torch.cat([real, c2], dim=1)\n",
        "                    else:\n",
        "                        real_cat = real\n",
        "                        fake_cat = fakeact\n",
        "\n",
        "                    y_fake = discriminator(fake_cat)\n",
        "                    y_real = discriminator(real_cat)\n",
        "\n",
        "                    pen = discriminator.calc_gradient_penalty(\n",
        "                        real_cat, fake_cat, self._device, self.pac\n",
        "                    )\n",
        "                    loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n",
        "\n",
        "                    optimizerD.zero_grad(set_to_none=False)\n",
        "                    pen.backward(retain_graph=True)\n",
        "                    loss_d.backward()\n",
        "                    optimizerD.step() #updates the dicriminator by applying the accumulated gradients from pen.backward and loss_d.backward\n",
        "\n",
        "                fakez = torch.normal(mean=mean, std=std)\n",
        "                condvec = self._data_sampler.sample_condvec(self._batch_size)\n",
        "\n",
        "                if condvec is None:\n",
        "                    c1, m1, col, opt = None, None, None, None\n",
        "                else:\n",
        "                    c1, m1, col, opt = condvec\n",
        "                    c1 = torch.from_numpy(c1).to(self._device)\n",
        "                    m1 = torch.from_numpy(m1).to(self._device)\n",
        "                    fakez = torch.cat([fakez, c1], dim=1)\n",
        "\n",
        "                fake = self._generator(fakez)\n",
        "                fakeact = self._apply_activate(fake)\n",
        "\n",
        "                if c1 is not None:\n",
        "                    y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n",
        "                else:\n",
        "                    y_fake = discriminator(fakeact)\n",
        "\n",
        "                if condvec is None:\n",
        "                    cross_entropy = 0\n",
        "                else:\n",
        "                    cross_entropy = self._cond_loss(fake, c1, m1)\n",
        "\n",
        "                loss_g = -torch.mean(y_fake) + cross_entropy\n",
        "\n",
        "                optimizerG.zero_grad(set_to_none=False)\n",
        "                loss_g.backward()\n",
        "                optimizerG.step()\n",
        "\n",
        "            generator_loss = loss_g.detach().cpu().item()\n",
        "            discriminator_loss = loss_d.detach().cpu().item()\n",
        "\n",
        "            if i%10==0:\n",
        "              print(f\"Gen Loss: {generator_loss}, Disc Loss: {discriminator_loss}\")\n",
        "\n",
        "            epoch_loss_df = pd.DataFrame({\n",
        "                'Epoch': [i],\n",
        "                'Generator Loss': [generator_loss],\n",
        "                'Discriminator Loss': [discriminator_loss],\n",
        "            })\n",
        "            if not self.loss_values.empty:\n",
        "                self.loss_values = pd.concat([self.loss_values, epoch_loss_df]).reset_index(\n",
        "                    drop=True\n",
        "                )\n",
        "            else:\n",
        "                self.loss_values = epoch_loss_df\n",
        "\n",
        "            if self._verbose:\n",
        "                epoch_iterator.set_description(\n",
        "                    description.format(gen=generator_loss, dis=discriminator_loss)\n",
        "                )\n",
        "\n",
        "    #random_state\n",
        "    def sample(self, n, condition_column=None, condition_value=None):\n",
        "        \"\"\"Sample data similar to the training data.\n",
        "\n",
        "        Choosing a condition_column and condition_value will increase the probability of the\n",
        "        discrete condition_value happening in the condition_column.\n",
        "\n",
        "        Args:\n",
        "            n (int):\n",
        "                Number of rows to sample.\n",
        "            condition_column (string):\n",
        "                Name of a discrete column.\n",
        "            condition_value (string):\n",
        "                Name of the category in the condition_column which we wish to increase the\n",
        "                probability of happening.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray or pandas.DataFrame\n",
        "        \"\"\"\n",
        "        if condition_column is not None and condition_value is not None:\n",
        "            condition_info = self._transformer.convert_column_name_value_to_id(\n",
        "                condition_column, condition_value\n",
        "            )\n",
        "            global_condition_vec = self._data_sampler.generate_cond_from_condition_column_info(\n",
        "                condition_info, self._batch_size\n",
        "            )\n",
        "        else:\n",
        "            global_condition_vec = None\n",
        "\n",
        "        steps = n // self._batch_size + 1\n",
        "        data = []\n",
        "        for i in range(steps):\n",
        "            mean = torch.zeros(self._batch_size, self._embedding_dim)\n",
        "            std = mean + 1\n",
        "            fakez = torch.normal(mean=mean, std=std).to(self._device)\n",
        "\n",
        "            if global_condition_vec is not None:\n",
        "                condvec = global_condition_vec.copy()\n",
        "            else:\n",
        "                condvec = self._data_sampler.sample_original_condvec(self._batch_size)\n",
        "\n",
        "            if condvec is None:\n",
        "                pass\n",
        "            else:\n",
        "                c1 = condvec\n",
        "                c1 = torch.from_numpy(c1).to(self._device)\n",
        "                fakez = torch.cat([fakez, c1], dim=1)\n",
        "\n",
        "            fake = self._generator(fakez)\n",
        "            fakeact = self._apply_activate(fake)\n",
        "            data.append(fakeact.detach().cpu().numpy())\n",
        "\n",
        "        data = np.concatenate(data, axis=0)\n",
        "        data = data[:n]\n",
        "\n",
        "        return self._transformer.inverse_transform(data)\n",
        "\n",
        "    def set_device(self, device):\n",
        "        \"\"\"Set the `device` to be used ('GPU' or 'CPU).\"\"\"\n",
        "        self._device = device\n",
        "        if self._generator is not None:\n",
        "            self._generator.to(self._device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctgan = CTGAN()\n",
        "\n",
        "ctgan.fit(train_data, discrete_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ornIUghq_IWb",
        "outputId": "3dc81bad-727a-4d08-ce07-2dc81d7fcb49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen Loss: 3.324735403060913, Disc Loss: 0.004792512860149145\n",
            "Gen Loss: 3.414158582687378, Disc Loss: -0.11552591621875763\n",
            "Gen Loss: 3.0535478591918945, Disc Loss: 0.030584610998630524\n",
            "Gen Loss: 2.989884853363037, Disc Loss: -0.01816660165786743\n",
            "Gen Loss: 3.089268684387207, Disc Loss: 0.12307192385196686\n",
            "Gen Loss: 2.687356948852539, Disc Loss: 0.06171485781669617\n",
            "Gen Loss: 2.917478322982788, Disc Loss: -0.09370441734790802\n",
            "Gen Loss: 2.396353244781494, Disc Loss: 0.00565752387046814\n",
            "Gen Loss: 2.53851580619812, Disc Loss: -0.06985197961330414\n",
            "Gen Loss: 1.8783351182937622, Disc Loss: 0.013525515794754028\n",
            "Gen Loss: 1.957963228225708, Disc Loss: 0.12787938117980957\n",
            "Gen Loss: 2.2943074703216553, Disc Loss: 0.07775095105171204\n",
            "Gen Loss: 1.8640459775924683, Disc Loss: -0.04981902241706848\n",
            "Gen Loss: 1.810501217842102, Disc Loss: 0.03661903738975525\n",
            "Gen Loss: 1.701981782913208, Disc Loss: 0.042858392000198364\n",
            "Gen Loss: 1.3961031436920166, Disc Loss: 0.023987680673599243\n",
            "Gen Loss: 1.374269962310791, Disc Loss: 0.0037319958209991455\n",
            "Gen Loss: 1.3340297937393188, Disc Loss: 0.008248358964920044\n",
            "Gen Loss: 1.0656801462173462, Disc Loss: -0.024713337421417236\n",
            "Gen Loss: 1.0229287147521973, Disc Loss: -0.010295957326889038\n",
            "Gen Loss: 0.8398497700691223, Disc Loss: -0.07758653163909912\n",
            "Gen Loss: 0.4807838499546051, Disc Loss: -0.11787965893745422\n",
            "Gen Loss: 0.8680863976478577, Disc Loss: -0.11063870787620544\n",
            "Gen Loss: 0.47298726439476013, Disc Loss: 0.09683868288993835\n",
            "Gen Loss: 0.5082874298095703, Disc Loss: -0.12672096490859985\n",
            "Gen Loss: 0.30902642011642456, Disc Loss: 0.2606721520423889\n",
            "Gen Loss: 0.3117969334125519, Disc Loss: -0.012356430292129517\n",
            "Gen Loss: 0.2588670253753662, Disc Loss: -0.19127997756004333\n",
            "Gen Loss: 0.27239900827407837, Disc Loss: -0.07510441541671753\n",
            "Gen Loss: 0.19255340099334717, Disc Loss: 0.02456188201904297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = ctgan.sample(100)"
      ],
      "metadata": {
        "id": "G2AacNCyeaJF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}