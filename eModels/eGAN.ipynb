{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import csv dataset"
      ],
      "metadata": {
        "id": "INx5D3LvOtbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()    # Upload the training data csv. The following implementation drops the null values. Custom handling before uploading is encouraged"
      ],
      "metadata": {
        "id": "RjV_bFfNuZOr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = '...'      # use the name .csv file imported above\n",
        "discrete_columns = []\n",
        "attributes_to_ignore = []     # attributes with unique values such as names and ids that dont hold any information for the data and make learning harder\n",
        "\n",
        "num_features = 15\n",
        "num_negatives = 5"
      ],
      "metadata": {
        "id": "Lt9TOXt4pY85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(dataset_name)\n",
        "train_data = train_data.dropna().reset_index(drop=True)\n",
        "train_data = train_data.drop(attributes_to_ignore, axis = 1)\n",
        "discrete_columns = [c for c in train_data.columns if c in discrete_columns]    # puts disrete_columns in the right order\n",
        "\n",
        "discrete_df = train_data[discrete_columns]"
      ],
      "metadata": {
        "id": "55HcYHtr-6rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n"
      ],
      "metadata": {
        "id": "JLg4H8KP-kMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class EmbeddingsNN(nn.Module):\n",
        "    def __init__(self, discrete_df, num_features, num_negatives, pairs_pc, batch_size = 4000):\n",
        "        super(EmbeddingsNN, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.num_epochs = 150\n",
        "        self.num_negatives = num_negatives\n",
        "        self.pairs_pc = pairs_pc\n",
        "        self.batch_size = batch_size\n",
        "        self.negtive_samples = []\n",
        "        self.discrete_columns = discrete_df.columns\n",
        "        self.num_words = self.compute_num_words(discrete_df)\n",
        "        self.num_features = num_features\n",
        "        self.f_matrix = self.get_f_matrix(self.word_occurance_pairs)\n",
        "        self.column_pairs= [\n",
        "        (col1, col2)\n",
        "        for col1 in self.discrete_columns\n",
        "        for col2 in self.discrete_columns\n",
        "        if col1 != col2\n",
        "        ]\n",
        "\n",
        "        # E is the matrix we want to learn\n",
        "        self.one_hot_matrix = torch.eye(self.num_words, device=self.device)\n",
        "        self.E = nn.Parameter(torch.empty(self.num_words, self.num_features, device=self.device))  # Matrix E (learnable)\n",
        "        self.theta = nn.Parameter(torch.empty(self.num_features, self.num_words, device=self.device))  # Vector Î¸ (learnable)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.E)\n",
        "        nn.init.xavier_uniform_(self.theta)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, idx_c, idx_t, batched=False):\n",
        "        #O = self.one_hot_matrix[idx_c]\n",
        "        #e_c = torch.matmul(O, self.E)\n",
        "        e_c = self.E[idx_c]\n",
        "        #e_c = torch.tanh(e_c)\n",
        "        theta_t = self.theta[:, idx_t]\n",
        "        #theta_t = torch.tanh(theta_t)\n",
        "        if batched:\n",
        "            return torch.sigmoid(torch.sum((e_c *(theta_t.T)), dim=1))\n",
        "        else:\n",
        "            return torch.sigmoid(torch.dot(e_c, theta_t))\n",
        "\n",
        "    def compute_num_words(self, discrete_df):\n",
        "        self.word_occurance_pairs = []\n",
        "        self.values_per_column = [0]*len(self.discrete_columns)\n",
        "        for column in self.discrete_columns:\n",
        "            value_counts_pairs = discrete_df[column].value_counts().items()\n",
        "            for v,c in value_counts_pairs:\n",
        "              self.word_occurance_pairs.append([v,c])\n",
        "              self.values_per_column[self.discrete_columns.get_loc(column)] += 1\n",
        "\n",
        "        self.words = [word for word, count in self.word_occurance_pairs]\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(self.words)}\n",
        "        return len(self.words)\n",
        "\n",
        "    def get_f_matrix(self, word_occurance_pairs):\n",
        "      total = sum(x[1] for x in word_occurance_pairs)\n",
        "      for i in range(len(word_occurance_pairs)):\n",
        "         word_occurance_pairs[i][1] = word_occurance_pairs[i][1]/total\n",
        "      sum_of_fs = sum(x[1]**(3/4) for x in word_occurance_pairs)\n",
        "      f_matrix =[]\n",
        "      for i in range(len(word_occurance_pairs)):\n",
        "          f_matrix.append([i, word_occurance_pairs[i][1]**(3/4)/sum_of_fs])\n",
        "      return f_matrix\n",
        "\n",
        "\n",
        "    def get_training_pairs(self, pairs_pc, total_batch):\n",
        "      num_pairs = int(len(self.column_pairs)*pairs_pc)\n",
        "      pairs_set = random.sample(self.column_pairs, k=num_pairs)\n",
        "      #pairs_set = self.column_pairs\n",
        "      training_pairs = []\n",
        "      for c, t in pairs_set:\n",
        "        centers = total_batch[c].map(self.word_to_idx)\n",
        "        targets = total_batch[t].map(self.word_to_idx)\n",
        "        pairs = torch.stack([\n",
        "            torch.tensor(centers.values, device=self.device),\n",
        "            torch.tensor(targets.values, device=self.device)\n",
        "        ], dim=1)\n",
        "        training_pairs.append(pairs)\n",
        "\n",
        "        # Concatenate all pairs from all column combinations\n",
        "      training_pairs = torch.cat(training_pairs, dim=0)\n",
        "      return training_pairs\n",
        "\n",
        "\n",
        "    def get_negative_samples(self, exclude_idx, num_negatives):\n",
        "      neg_samples = []\n",
        "      numbers, probabilities = zip(*self.f_matrix)\n",
        "      for _ in range(num_negatives):\n",
        "          r = random.choices(numbers, weights=probabilities, k=len(exclude_idx))\n",
        "          neg_samples.append(r)\n",
        "      return torch.tensor(neg_samples, device = self.device).T\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        num_samples = len(discrete_df)\n",
        "        counter = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            total_loss = 0\n",
        "\n",
        "            for i in range(0, num_samples, self.batch_size):\n",
        "                counter+=1\n",
        "                batch_df = discrete_df.iloc[i:i+self.batch_size]\n",
        "                if len(batch_df) == 0:\n",
        "                    continue\n",
        "\n",
        "                training_pairs = self.get_training_pairs(self.pairs_pc, batch_df)\n",
        "                targets = training_pairs[:, 1]\n",
        "                negative_samples = self.get_negative_samples(targets, self.num_negatives)\n",
        "\n",
        "                tp_exp = training_pairs[:, 0].repeat_interleave(negative_samples.shape[1])\n",
        "                negative_samples = negative_samples.flatten()\n",
        "                negative_samples = torch.stack([tp_exp, negative_samples], dim=1)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                pos_scores = self.forward(training_pairs[:,0], training_pairs[:,1], batched=True)\n",
        "                pos_loss = self.criterion(pos_scores, torch.ones_like(pos_scores))\n",
        "\n",
        "                neg_scores = self.forward(negative_samples[:,0], negative_samples[:,1], batched=True)\n",
        "                neg_loss = self.criterion(neg_scores, torch.zeros_like(neg_scores))\n",
        "\n",
        "                loss = (pos_loss + neg_loss)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            if epoch%10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1VUEcir3-t12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_prob = 0.3  # the probability of selecting a positive sample for training in a given epoch\n",
        "ebd = EmbeddingsNN(discrete_df, num_features, num_negatives, pair_prob)\n",
        "ebd.train()"
      ],
      "metadata": {
        "id": "PmWeUm9k-zSQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E = torch.tanh(ebd.E.T).detach().cpu().numpy()\n",
        "embeddings_df = pd.DataFrame(E, columns = ebd.words)"
      ],
      "metadata": {
        "id": "SHA09gqGWmOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df = (embeddings_df - embeddings_df.mean()) / embeddings_df.std()"
      ],
      "metadata": {
        "id": "5xsbnnqx6w65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the values of the embeddings\n",
        "max = embeddings_df.abs().max().max()\n",
        "embeddings_df = (embeddings_df / max).astype('float32')\n",
        "\n",
        "#apply embeddings on words\n",
        "def word_to_vec(word):\n",
        "    if word in embeddings_df.columns.tolist():\n",
        "       return embeddings_df[word].values\n",
        "\n",
        "#construct the new dataframe with continuous ad embedded columns\n",
        "def construct_dataset(train_df):\n",
        "    i=0\n",
        "    _df = train_df.copy()\n",
        "    cat_col_pos = []\n",
        "    for column in _df:\n",
        "       if column in discrete_columns:\n",
        "          cat_col_pos.append(column)\n",
        "          _df[column] = _df[column].apply(lambda x: word_to_vec(x))\n",
        "       else:\n",
        "          cat_col_pos.append(None)\n",
        "    return _df, cat_col_pos\n",
        "\n",
        "new_df, cat_cols_pos = construct_dataset(train_data)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZSWQ6vxXAogh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_rows = []\n",
        "for _, row in new_df.iterrows():\n",
        "    flat = []\n",
        "    for val in row:\n",
        "        if isinstance(val, (list, np.ndarray)):\n",
        "            flat.extend(val)  # unpack lists or arrays\n",
        "        else:\n",
        "            flat.append(val)  # keep scalar values\n",
        "    new_rows.append(flat)\n",
        "\n",
        "new_rows = np.asarray(new_rows)"
      ],
      "metadata": {
        "id": "hs2GB-gRN4_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eGAN"
      ],
      "metadata": {
        "id": "i4xKZ8Fy2gQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional\n",
        "from torch.autograd import grad\n",
        "\n",
        "\n",
        "class Residual(Module):\n",
        "    \"\"\"Residual layer for the CTGAN.\"\"\"\n",
        "\n",
        "    def __init__(self, i, o):\n",
        "        super(Residual, self).__init__()\n",
        "        self.fc = Linear(i, o)\n",
        "        self.bn = BatchNorm1d(o)\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def forward(self, input_):\n",
        "        \"\"\"Apply the Residual layer to the `input_`.\"\"\"\n",
        "        out = self.fc(input_)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==== Generator ====\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, embedding_dim, generator_dims, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        dim = embedding_dim\n",
        "        seq = []\n",
        "        for item in list(generator_dims):\n",
        "            seq += [Residual(dim, item)]\n",
        "            dim = item\n",
        "        seq.append(Linear(dim, data_dim))\n",
        "        self.seq = Sequential(*seq)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        \"\"\"Apply the Generator to the `input_`.\"\"\"\n",
        "        data = self.seq(input_)\n",
        "        return data\n",
        "\n",
        "\n",
        "# ==== Discriminator ====\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2), #nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2), #nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2), #nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            #nn.Sigmoid()#nn.Tanh()#nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def compute_gradient_penalty(critic, real_data, fake_data, device):\n",
        "    batch_size = real_data.size(0)\n",
        "    epsilon = torch.rand(batch_size, 1, device=device)\n",
        "    epsilon = epsilon.expand_as(real_data)\n",
        "\n",
        "    interpolated = epsilon * real_data + (1 - epsilon) * fake_data\n",
        "    interpolated.requires_grad_(True)\n",
        "\n",
        "    prob_interpolated = critic(interpolated)\n",
        "    gradients = grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                     grad_outputs=torch.ones_like(prob_interpolated),\n",
        "                     create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    gradients = gradients.view(batch_size, -1)\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "    return penalty*2\n",
        "\n",
        "\n",
        "class eGAN():\n",
        "    def __init__(self, embedding_dim=128, generator_dims=(256,512, 1024, 512, 256), discriminator_dim=256, discriminator_steps=2, epochs=600, batch_size=2000, cuda=True):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.generator_dims = generator_dims\n",
        "        self.discriminator_dim = discriminator_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.discriminator_steps = discriminator_steps\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "        if not cuda or not torch.cuda.is_available():\n",
        "            device = 'cpu'\n",
        "        elif isinstance(cuda, str):\n",
        "            device = cuda\n",
        "        else:\n",
        "            device = 'cuda'\n",
        "\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.data_dim = data.shape[1]\n",
        "        self.data = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        self.generator = Generator(self.embedding_dim, self.generator_dims, self.data_dim).to(self.device)\n",
        "        self.discriminator = Discriminator(self.data_dim, self.discriminator_dim).to(self.device)\n",
        "\n",
        "        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
        "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=1e-5, betas=(0.5, 0.9))\n",
        "\n",
        "        data_loader = torch.utils.data.DataLoader(self.data, batch_size=self.batch_size)\n",
        "\n",
        "        fixed_real_labels = torch.ones((self.batch_size, 1), device=self.device)\n",
        "        fixed_fake_labels = torch.zeros((self.batch_size, 1), device=self.device)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            for real_batch in data_loader:\n",
        "                batch_size_curr = real_batch.size(0)\n",
        "                real_labels = fixed_real_labels[:batch_size_curr]\n",
        "                fake_labels = fixed_fake_labels[:batch_size_curr]\n",
        "\n",
        "                #Train Discriminator\n",
        "                for i in range(self.discriminator_steps):\n",
        "                    d_real = self.discriminator(real_batch)\n",
        "\n",
        "                    z = torch.randn(batch_size_curr, self.embedding_dim, device=self.device)\n",
        "                    fake_data = self.generator(z)\n",
        "                    d_fake = self.discriminator(fake_data.detach())\n",
        "\n",
        "                    loss_real = -torch.mean(d_real) #WGAN\n",
        "                    loss_fake = torch.mean(d_fake) #WGAN\n",
        "\n",
        "                    gp = compute_gradient_penalty(self.discriminator, real_batch, fake_data.detach(), self.device)\n",
        "\n",
        "                    loss_D = loss_real + loss_fake + 0.1*torch.abs(torch.var(d_real) - torch.var(d_fake))\n",
        "\n",
        "                    self.optimizer_D.zero_grad()\n",
        "                    gp.backward(retain_graph=True)\n",
        "                    loss_D.backward()\n",
        "                    self.optimizer_D.step()\n",
        "\n",
        "                #Train Generator\n",
        "                z = torch.randn(batch_size_curr, self.embedding_dim, device=self.device)\n",
        "                generated_data = self.generator(z)\n",
        "                outputs = self.discriminator(generated_data)\n",
        "\n",
        "                loss_G = -torch.mean(outputs)\n",
        "\n",
        "                self.optimizer_G.zero_grad()\n",
        "                loss_G.backward()\n",
        "                self.optimizer_G.step()\n",
        "\n",
        "            if epoch % 25 == 0:\n",
        "                print(f\"Epoch {epoch} | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f}\")\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(num_samples, self.embedding_dim, device=self.device)\n",
        "            generated_data = self.generator(z)\n",
        "        return generated_data.cpu().numpy()"
      ],
      "metadata": {
        "id": "yJwK-rb12HJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "l_nr = np.log1p(new_rows+0.01)  #For smoothing. It compresses large values and expands small ones\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(l_nr)\n",
        "\n",
        "egan = eGAN()\n",
        "egan.fit(X_scaled)"
      ],
      "metadata": {
        "id": "v2oP0SwN2un4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discrete_values(tmp, start, end):\n",
        "    X = embeddings_df.iloc[:, start : end]\n",
        "    res=[]\n",
        "    score = float('inf')\n",
        "    for x in tmp:\n",
        "        att=None\n",
        "        score = float('inf')\n",
        "        for v in X:\n",
        "            s = nn.CosineEmbeddingLoss()(torch.tensor(X[v].values).unsqueeze(0), torch.tensor(x).unsqueeze(0), torch.ones(x.shape))\n",
        "            if (s<score):\n",
        "               score=s\n",
        "               att=v\n",
        "        res.append(att)\n",
        "    return res\n",
        "\n",
        "def create_final_df(sample, columns, discrete_columns, num_features):\n",
        "    final_df = pd.DataFrame()\n",
        "    i=0\n",
        "    j=0\n",
        "    col_atts_indeces = np.concatenate([np.zeros(1).astype(int), np.cumsum(ebd.values_per_column)])\n",
        "    for c in columns:\n",
        "        if c in discrete_columns:\n",
        "            tmp = sample[:,i:i+num_features]\n",
        "            attributes = get_discrete_values(tmp, col_atts_indeces[j], col_atts_indeces[j+1])\n",
        "            j+=1\n",
        "            final_df[c] = attributes\n",
        "            i+=num_features\n",
        "        else:\n",
        "            final_df[c] = sample[:,i]\n",
        "            i+=1\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "OZ_ZsMab38sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = egan.sample(2000)\n",
        "\n",
        "samples = scaler.inverse_transform(d)\n",
        "samples = np.expm1(samples)"
      ],
      "metadata": {
        "id": "5eg5g_5u3BGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = create_final_df(samples, new_df.columns, discrete_columns, num_features)"
      ],
      "metadata": {
        "id": "TfSiUUvlVCii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "id": "hrgv1u-1r3Ai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}