{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import csv dataset\n"
      ],
      "metadata": {
        "id": "nPvf8tDbM1S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "tqy5kPWQ3oC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = '...'      # use the name .csv file imported above\n",
        "discrete_columns = ['...']\n",
        "attributes_to_ignore = ['...']     # attributes with unique values such as names and ids that dont hold any information for the data and make learning harder\n",
        "\n",
        "num_features = 15\n",
        "num_negatives = 5"
      ],
      "metadata": {
        "id": "4wcxEn3Z3oA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(dataset_name)\n",
        "train_data = train_data.dropna().reset_index(drop=True)\n",
        "train_data = train_data.drop(attributes_to_ignore, axis = 1)\n",
        "discrete_columns = [c for c in train_data.columns if c in discrete_columns]    # puts disrete_columns in the right order\n",
        "\n",
        "discrete_df = train_data[discrete_columns]"
      ],
      "metadata": {
        "id": "6PMvK7AA3n-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n"
      ],
      "metadata": {
        "id": "JLg4H8KP-kMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class EmbeddingsNN(nn.Module):\n",
        "    def __init__(self, discrete_df, num_features, num_negatives, pairs_pc, batch_size = 4000):\n",
        "        super(EmbeddingsNN, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.num_epochs = 150\n",
        "        self.num_negatives = num_negatives\n",
        "        self.pairs_pc = pairs_pc\n",
        "        self.batch_size = batch_size\n",
        "        self.negtive_samples = []\n",
        "        self.discrete_columns = discrete_df.columns\n",
        "        self.num_words = self.compute_num_words(discrete_df)\n",
        "        self.num_features = num_features\n",
        "        self.f_matrix = self.get_f_matrix(self.word_occurance_pairs)\n",
        "        self.column_pairs= [\n",
        "        (col1, col2)\n",
        "        for col1 in self.discrete_columns\n",
        "        for col2 in self.discrete_columns\n",
        "        if col1 != col2\n",
        "        ]\n",
        "\n",
        "        # E is the matrix we want to learn\n",
        "        self.one_hot_matrix = torch.eye(self.num_words, device=self.device)\n",
        "        self.E = nn.Parameter(torch.empty(self.num_words, self.num_features, device=self.device))  # Matrix E (learnable)\n",
        "        self.theta = nn.Parameter(torch.empty(self.num_features, self.num_words, device=self.device))  # Vector Î¸ (learnable)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.E)\n",
        "        nn.init.xavier_uniform_(self.theta)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, idx_c, idx_t, batched=False):\n",
        "        #O = self.one_hot_matrix[idx_c]\n",
        "        #e_c = torch.matmul(O, self.E)\n",
        "        e_c = self.E[idx_c]\n",
        "        #e_c = torch.tanh(e_c)\n",
        "        theta_t = self.theta[:, idx_t]\n",
        "        #theta_t = torch.tanh(theta_t)\n",
        "        if batched:\n",
        "            return torch.sigmoid(torch.sum((e_c *(theta_t.T)), dim=1))\n",
        "        else:\n",
        "            return torch.sigmoid(torch.dot(e_c, theta_t))\n",
        "\n",
        "    def compute_num_words(self, discrete_df):\n",
        "        self.word_occurance_pairs = []\n",
        "        self.values_per_column = [0]*len(self.discrete_columns)\n",
        "        for column in self.discrete_columns:\n",
        "            value_counts_pairs = discrete_df[column].value_counts().items()\n",
        "            for v,c in value_counts_pairs:\n",
        "              self.word_occurance_pairs.append([v,c])\n",
        "              self.values_per_column[self.discrete_columns.get_loc(column)] += 1\n",
        "\n",
        "        self.words = [word for word, count in self.word_occurance_pairs]\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(self.words)}\n",
        "        return len(self.words)\n",
        "\n",
        "    def get_f_matrix(self, word_occurance_pairs):\n",
        "      total = sum(x[1] for x in word_occurance_pairs)\n",
        "      for i in range(len(word_occurance_pairs)):\n",
        "         word_occurance_pairs[i][1] = word_occurance_pairs[i][1]/total\n",
        "      sum_of_fs = sum(x[1]**(3/4) for x in word_occurance_pairs)\n",
        "      f_matrix =[]\n",
        "      for i in range(len(word_occurance_pairs)):\n",
        "          f_matrix.append([i, word_occurance_pairs[i][1]**(3/4)/sum_of_fs])\n",
        "      return f_matrix\n",
        "\n",
        "\n",
        "    def get_training_pairs(self, pairs_pc, total_batch):\n",
        "      num_pairs = int(len(self.column_pairs)*pairs_pc)\n",
        "      pairs_set = random.sample(self.column_pairs, k=num_pairs)\n",
        "      #pairs_set = self.column_pairs\n",
        "      training_pairs = []\n",
        "      for c, t in pairs_set:\n",
        "        centers = total_batch[c].map(self.word_to_idx)\n",
        "        targets = total_batch[t].map(self.word_to_idx)\n",
        "        pairs = torch.stack([\n",
        "            torch.tensor(centers.values, device=self.device),\n",
        "            torch.tensor(targets.values, device=self.device)\n",
        "        ], dim=1)\n",
        "        training_pairs.append(pairs)\n",
        "\n",
        "        # Concatenate all pairs from all column combinations\n",
        "      training_pairs = torch.cat(training_pairs, dim=0)\n",
        "      return training_pairs\n",
        "\n",
        "\n",
        "    def get_negative_samples(self, exclude_idx, num_negatives):\n",
        "      neg_samples = []\n",
        "      numbers, probabilities = zip(*self.f_matrix)\n",
        "      for _ in range(num_negatives):\n",
        "          r = random.choices(numbers, weights=probabilities, k=len(exclude_idx))\n",
        "          neg_samples.append(r)\n",
        "      return torch.tensor(neg_samples, device = self.device).T\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        num_samples = len(discrete_df)\n",
        "        counter = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            total_loss = 0\n",
        "\n",
        "            for i in range(0, num_samples, self.batch_size):\n",
        "                counter+=1\n",
        "                batch_df = discrete_df.iloc[i:i+self.batch_size]\n",
        "                if len(batch_df) == 0:\n",
        "                    continue\n",
        "\n",
        "                training_pairs = self.get_training_pairs(self.pairs_pc, batch_df)\n",
        "                targets = training_pairs[:, 1]\n",
        "                negative_samples = self.get_negative_samples(targets, self.num_negatives)\n",
        "\n",
        "                tp_exp = training_pairs[:, 0].repeat_interleave(negative_samples.shape[1])\n",
        "                negative_samples = negative_samples.flatten()\n",
        "                negative_samples = torch.stack([tp_exp, negative_samples], dim=1)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                pos_scores = self.forward(training_pairs[:,0], training_pairs[:,1], batched=True)\n",
        "                pos_loss = self.criterion(pos_scores, torch.ones_like(pos_scores))\n",
        "\n",
        "                neg_scores = self.forward(negative_samples[:,0], negative_samples[:,1], batched=True)\n",
        "                neg_loss = self.criterion(neg_scores, torch.zeros_like(neg_scores))\n",
        "\n",
        "                loss = (pos_loss + neg_loss)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            if epoch%10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "1VUEcir3-t12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_prob = 0.3  # the probability of selecting a positive sample for training in a given epoch\n",
        "ebd = EmbeddingsNN(discrete_df, num_features, num_negatives, pair_prob)\n",
        "ebd.train()"
      ],
      "metadata": {
        "id": "PmWeUm9k-zSQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E = torch.tanh(ebd.E.T).detach().cpu().numpy()\n",
        "embeddings_df = pd.DataFrame(E, columns = ebd.words)"
      ],
      "metadata": {
        "id": "SHA09gqGWmOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df = (embeddings_df - embeddings_df.mean()) / embeddings_df.std()"
      ],
      "metadata": {
        "id": "5xsbnnqx6w65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the values of the embeddings\n",
        "max = embeddings_df.abs().max().max()\n",
        "embeddings_df = (embeddings_df / max).astype('float32')\n",
        "\n",
        "#apply embeddings on words\n",
        "def word_to_vec(word):\n",
        "    if word in embeddings_df.columns.tolist():\n",
        "       return embeddings_df[word].values\n",
        "\n",
        "#construct the new dataframe with continuous ad embedded columns\n",
        "def construct_dataset(_df):\n",
        "    i=0\n",
        "    cat_col_pos = []\n",
        "    for column in _df:\n",
        "       if column in discrete_columns:\n",
        "          cat_col_pos.append(column)\n",
        "          _df[column] = _df[column].apply(lambda x: word_to_vec(x))\n",
        "       else:\n",
        "          cat_col_pos.append(None)\n",
        "    return _df, cat_col_pos\n",
        "\n",
        "new_df, cat_cols_pos = construct_dataset(train_data.copy())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZSWQ6vxXAogh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_rows = []\n",
        "for _, row in new_df.iterrows():\n",
        "    flat = []\n",
        "    for val in row:\n",
        "        if isinstance(val, (list, np.ndarray)):\n",
        "            flat.extend(val)  # unpack lists or arrays\n",
        "        else:\n",
        "            flat.append(val)  # keep scalar values\n",
        "    new_rows.append(flat)\n",
        "\n",
        "new_rows = np.array(new_rows)"
      ],
      "metadata": {
        "id": "hs2GB-gRN4_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DDPM"
      ],
      "metadata": {
        "id": "3_VgnwDGuRqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baesd on Algorithm 1 (Trainings) and Algorithm 2 (Sampling) of https://arxiv.org/pdf/2006.11239 at page 4"
      ],
      "metadata": {
        "id": "EwNQocnPkKgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "class DenoiseMLP(nn.Module):\n",
        "    def __init__(self, data_dim, time_dim):\n",
        "        super().__init__()\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(1, time_dim), nn.ReLU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(data_dim + time_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(1024),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(1024),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Linear(512, data_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t = t.view(-1, 1).float() / 1000  # Normalize timestep\n",
        "        time_emb = self.time_embed(t)\n",
        "        x = torch.cat([x, time_emb], dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class eDDPM(nn.Module):\n",
        "    def __init__(self, data_dim, time_dim=128, T=400, lr=1e-4, cuda=True):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.lr = lr\n",
        "        self.data_dim = data_dim\n",
        "\n",
        "        if not cuda or not torch.cuda.is_available():\n",
        "            self.device = 'cpu'\n",
        "        elif isinstance(cuda, str):\n",
        "            self.device = cuda\n",
        "        else:\n",
        "            self.device = 'cuda'\n",
        "\n",
        "        self.betas = torch.linspace(1e-4, 0.02, T).to(self.device)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod).to(self.device)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "\n",
        "        self.device = torch.device(self.device)\n",
        "        self.denoiser = DenoiseMLP(data_dim, time_dim).to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def q_sample(self, x0, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "        return self.sqrt_alphas_cumprod[t].unsqueeze(1) * x0 + self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(1) * noise\n",
        "\n",
        "    def train(self, data, num_epochs=1000, batch_size=4000):\n",
        "        data = torch.from_numpy(data).float().to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        data_loader = torch.utils.data.DataLoader(data, batch_size)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for batch in data_loader:\n",
        "                #t = torch.randint(0, self.T, (batch.shape[0],), dtype=torch.long).to(self.device)\n",
        "                weights = torch.linspace(0.1, 1.0, self.T)  # increasing weight\n",
        "                t = torch.multinomial(weights, batch.shape[0], replacement=True).to(self.device)\n",
        "                noise = torch.randn_like(batch)\n",
        "                xt = self.q_sample(batch, t, noise)\n",
        "\n",
        "                pred_noise = self.denoiser(xt, t)\n",
        "                loss = F.mse_loss(pred_noise, noise) #/ torch.sqrt(self.alphas[t].mean())\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                #torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            if epoch % 20 == 0:\n",
        "                print(f\"Epoch {epoch}: loss {loss.item():.4f}\")\n",
        "\n",
        "    #@torch.no_grad()\n",
        "    def sample(self, num_samples):\n",
        "        with torch.no_grad():\n",
        "            x_t = torch.randn((num_samples, self.data_dim), device = self.device)\n",
        "            for t in reversed(range(self.T)):\n",
        "                t_tensor = torch.tensor([t]*num_samples, device=self.device)\n",
        "                pred_noise = self.denoiser(x_t, t_tensor)\n",
        "                beta_t = self.betas[t]\n",
        "                alpha_t = self.alphas[t]\n",
        "                alpha_bar_t = self.alphas_cumprod[t]\n",
        "                sigmas = torch.sqrt(beta_t) # or use sigmas = torch.sqrt(1 - self.alphas[t]) given by chat\n",
        "\n",
        "                x_t_minus_one = (1 / torch.sqrt(alpha_t)) * (x_t - (1-alpha_t) / torch.sqrt(1 - alpha_bar_t) * pred_noise)\n",
        "                x_t = x_t_minus_one\n",
        "                if t > 0:\n",
        "                    noise = torch.randn_like(x_t)\n",
        "                    x_t += sigmas * noise\n",
        "            return x_t"
      ],
      "metadata": {
        "id": "QC104lfx7byp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "l_nr = np.log1p(new_rows+0.01)  #For smoothing. It compresses large values and expands small ones\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(l_nr)\n",
        "\n",
        "eddpm = eDDPM(l_nr.shape[1])\n",
        "eddpm.train(X_scaled)"
      ],
      "metadata": {
        "id": "uUdY-LpfwNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = eddpm.sample(5000)\n",
        "d = d.detach().cpu().numpy()\n",
        "\n",
        "samples = scaler.inverse_transform(d)\n",
        "samples = np.expm1(samples)"
      ],
      "metadata": {
        "id": "94kTTcsA4BID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discrete_values(tmp, start, end):\n",
        "    X = embeddings_df.iloc[:, start : end]\n",
        "    res=[]\n",
        "    score = float('inf')\n",
        "    for x in tmp:\n",
        "        att=None\n",
        "        score = float('inf')\n",
        "        for v in X:\n",
        "            s = nn.CosineEmbeddingLoss()(torch.tensor(X[v].values).unsqueeze(0), torch.tensor(x).unsqueeze(0), torch.ones(x.shape))\n",
        "            if (s<score):\n",
        "               score=s\n",
        "               att=v\n",
        "        res.append(att)\n",
        "    return res\n",
        "\n",
        "\n",
        "def create_final_df(sample, columns, discrete_columns, num_features):\n",
        "    final_df = pd.DataFrame()\n",
        "    i=0\n",
        "    j=0\n",
        "    col_atts_indeces = np.concatenate([np.zeros(1).astype(int), np.cumsum(ebd.values_per_column)])\n",
        "    for c in columns:\n",
        "        if c in discrete_columns:\n",
        "            tmp = sample[:,i:i+num_features]\n",
        "            attributes = get_discrete_values(tmp, col_atts_indeces[j], col_atts_indeces[j+1])\n",
        "            j+=1\n",
        "            final_df[c] = attributes\n",
        "            i+=num_features\n",
        "        else:\n",
        "            final_df[c] = sample[:,i]\n",
        "            i+=1\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "1FYtD9Lcwjub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = create_final_df(samples, new_df.columns, discrete_columns, num_features)"
      ],
      "metadata": {
        "id": "kaqKT0khLIY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "id": "W55lxZog1Uns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oENqaW50PO7P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}