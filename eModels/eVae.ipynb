{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "uLFCqOKIOUuA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import csv dataset\n"
      ],
      "metadata": {
        "id": "uLFCqOKIOUuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()    # Upload the training data csv. The following implementation drops the null values. Custom handling before uploading is encouraged"
      ],
      "metadata": {
        "collapsed": true,
        "id": "s4I6p0Y9nJvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = '...'      # use the name .csv file imported above\n",
        "discrete_columns = [ ]\n",
        "attributes_to_ignore = []     # attributes with unique values such as names and ids that dont hold any information for the data and make learning harder\n",
        "\n",
        "num_features = 15\n",
        "num_negatives = 5"
      ],
      "metadata": {
        "id": "CJv4da-BnPBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(dataset_name)\n",
        "train_data = train_data.dropna().reset_index(drop=True)\n",
        "train_data = train_data.drop(attributes_to_ignore, axis = 1)\n",
        "discrete_columns = [c for c in train_data.columns if c in discrete_columns]    # puts disrete_columns in the right order\n",
        "\n",
        "discrete_df = train_data[discrete_columns]"
      ],
      "metadata": {
        "id": "55HcYHtr-6rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n"
      ],
      "metadata": {
        "id": "JLg4H8KP-kMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class EmbeddingsNN(nn.Module):\n",
        "    def __init__(self, discrete_df, num_features, num_negatives, pairs_pc, batch_size = 4000):\n",
        "        super(EmbeddingsNN, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.num_epochs = 150\n",
        "        self.num_negatives = num_negatives\n",
        "        self.pairs_pc = pairs_pc\n",
        "        self.batch_size = batch_size\n",
        "        self.negtive_samples = []\n",
        "        self.discrete_columns = discrete_df.columns\n",
        "        self.num_words = self.compute_num_words(discrete_df)\n",
        "        self.num_features = num_features\n",
        "        self.f_matrix = self.get_f_matrix(self.word_occurance_pairs)\n",
        "        self.column_pairs= [\n",
        "        (col1, col2)\n",
        "        for col1 in self.discrete_columns\n",
        "        for col2 in self.discrete_columns\n",
        "        if col1 != col2\n",
        "        ]\n",
        "\n",
        "        # E is the matrix we want to learn\n",
        "        self.one_hot_matrix = torch.eye(self.num_words, device=self.device)\n",
        "        self.E = nn.Parameter(torch.empty(self.num_words, self.num_features, device=self.device))  # Matrix E (learnable)\n",
        "        self.theta = nn.Parameter(torch.empty(self.num_features, self.num_words, device=self.device))  # Vector Î¸ (learnable)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.E)\n",
        "        nn.init.xavier_uniform_(self.theta)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, idx_c, idx_t, batched=False):\n",
        "        #O = self.one_hot_matrix[idx_c]\n",
        "        #e_c = torch.matmul(O, self.E)\n",
        "        e_c = self.E[idx_c]\n",
        "        #e_c = torch.tanh(e_c)\n",
        "        theta_t = self.theta[:, idx_t]\n",
        "        #theta_t = torch.tanh(theta_t)\n",
        "        if batched:\n",
        "            return torch.sigmoid(torch.sum((e_c *(theta_t.T)), dim=1))\n",
        "        else:\n",
        "            return torch.sigmoid(torch.dot(e_c, theta_t))\n",
        "\n",
        "    def compute_num_words(self, discrete_df):\n",
        "        self.word_occurance_pairs = []\n",
        "        self.values_per_column = [0]*len(self.discrete_columns)\n",
        "        for column in self.discrete_columns:\n",
        "            value_counts_pairs = discrete_df[column].value_counts().items()\n",
        "            for v,c in value_counts_pairs:\n",
        "              self.word_occurance_pairs.append([v,c])\n",
        "              self.values_per_column[self.discrete_columns.get_loc(column)] += 1\n",
        "\n",
        "        self.words = [word for word, count in self.word_occurance_pairs]\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(self.words)}\n",
        "        return len(self.words)\n",
        "\n",
        "    def get_f_matrix(self, word_occurance_pairs):\n",
        "      total = sum(x[1] for x in word_occurance_pairs)\n",
        "      for i in range(len(word_occurance_pairs)):\n",
        "         word_occurance_pairs[i][1] = word_occurance_pairs[i][1]/total\n",
        "      sum_of_fs = sum(x[1]**(3/4) for x in word_occurance_pairs)\n",
        "      f_matrix =[]\n",
        "      for i in range(len(word_occurance_pairs)):\n",
        "          f_matrix.append([i, word_occurance_pairs[i][1]**(3/4)/sum_of_fs])\n",
        "      return f_matrix\n",
        "\n",
        "\n",
        "    def get_training_pairs(self, pairs_pc, total_batch):\n",
        "      num_pairs = int(len(self.column_pairs)*pairs_pc)\n",
        "      pairs_set = random.sample(self.column_pairs, k=num_pairs)\n",
        "      #pairs_set = self.column_pairs\n",
        "      training_pairs = []\n",
        "      for c, t in pairs_set:\n",
        "        centers = total_batch[c].map(self.word_to_idx)\n",
        "        targets = total_batch[t].map(self.word_to_idx)\n",
        "        pairs = torch.stack([\n",
        "            torch.tensor(centers.values, device=self.device),\n",
        "            torch.tensor(targets.values, device=self.device)\n",
        "        ], dim=1)\n",
        "        training_pairs.append(pairs)\n",
        "\n",
        "        # Concatenate all pairs from all column combinations\n",
        "      training_pairs = torch.cat(training_pairs, dim=0)\n",
        "      return training_pairs\n",
        "\n",
        "\n",
        "    def get_negative_samples(self, exclude_idx, num_negatives):\n",
        "      neg_samples = []\n",
        "      numbers, probabilities = zip(*self.f_matrix)\n",
        "      for _ in range(num_negatives):\n",
        "          r = random.choices(numbers, weights=probabilities, k=len(exclude_idx))\n",
        "          neg_samples.append(r)\n",
        "      return torch.tensor(neg_samples, device = self.device).T\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        num_samples = len(discrete_df)\n",
        "        counter = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            total_loss = 0\n",
        "\n",
        "            for i in range(0, num_samples, self.batch_size):\n",
        "                counter+=1\n",
        "                batch_df = discrete_df.iloc[i:i+self.batch_size]\n",
        "                if len(batch_df) == 0:\n",
        "                    continue\n",
        "\n",
        "                training_pairs = self.get_training_pairs(self.pairs_pc, batch_df)\n",
        "                targets = training_pairs[:, 1]\n",
        "                negative_samples = self.get_negative_samples(targets, self.num_negatives)\n",
        "\n",
        "                tp_exp = training_pairs[:, 0].repeat_interleave(negative_samples.shape[1])\n",
        "                negative_samples = negative_samples.flatten()\n",
        "                negative_samples = torch.stack([tp_exp, negative_samples], dim=1)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                pos_scores = self.forward(training_pairs[:,0], training_pairs[:,1], batched=True)\n",
        "                pos_loss = self.criterion(pos_scores, torch.ones_like(pos_scores))\n",
        "\n",
        "                neg_scores = self.forward(negative_samples[:,0], negative_samples[:,1], batched=True)\n",
        "                neg_loss = self.criterion(neg_scores, torch.zeros_like(neg_scores))\n",
        "\n",
        "                loss = (pos_loss + neg_loss)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            if epoch%10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "1VUEcir3-t12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_prob = 0.3  # the probability of selecting a positive sample for training in a given epoch\n",
        "ebd = EmbeddingsNN(discrete_df, num_features, num_negatives, pair_prob)\n",
        "ebd.train()"
      ],
      "metadata": {
        "id": "PmWeUm9k-zSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E = torch.tanh(ebd.E.T).detach().cpu().numpy()\n",
        "embeddings_df = pd.DataFrame(E, columns = ebd.words)"
      ],
      "metadata": {
        "id": "SHA09gqGWmOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df = (embeddings_df - embeddings_df.mean()) / embeddings_df.std()"
      ],
      "metadata": {
        "id": "5xsbnnqx6w65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the values of the embeddings\n",
        "max = embeddings_df.abs().max().max()\n",
        "embeddings_df = (embeddings_df / max).astype('float32')\n",
        "\n",
        "#apply embeddings on words\n",
        "def word_to_vec(word):\n",
        "    if word in embeddings_df.columns.tolist():\n",
        "       return embeddings_df[word].values\n",
        "\n",
        "#construct the new dataframe with continuous ad embedded columns\n",
        "def construct_dataset(_df):\n",
        "    i=0\n",
        "    cat_col_pos = []\n",
        "    for column in _df:\n",
        "       if column in discrete_columns:\n",
        "          cat_col_pos.append(column)\n",
        "          _df[column] = _df[column].apply(lambda x: word_to_vec(x))\n",
        "       else:\n",
        "          cat_col_pos.append(None)\n",
        "    return _df, cat_col_pos\n",
        "\n",
        "new_df, cat_cols_pos = construct_dataset(train_data)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZSWQ6vxXAogh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_rows = []\n",
        "for _, row in new_df.iterrows():\n",
        "    flat = []\n",
        "    for val in row:\n",
        "        if isinstance(val, (list, np.ndarray)):\n",
        "            flat.extend(val)  # unpack lists or arrays\n",
        "        else:\n",
        "            flat.append(val)  # keep scalar values\n",
        "    new_rows.append(flat)\n",
        "\n",
        "new_rows = np.array(new_rows)"
      ],
      "metadata": {
        "id": "hs2GB-gRN4_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "MLMC5xwg-prw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdt tqdm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3H_c2_Tu9MLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV0-helZad9o"
      },
      "outputs": [],
      "source": [
        "\"\"\"TVAE module.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import Linear, Module, Parameter, ReLU, Sequential, Dropout\n",
        "from torch.nn.functional import cross_entropy\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from transformer import DataTransformer\n",
        "\n",
        "\n",
        "class Encoder(Module):\n",
        "    \"\"\"Encoder for the TVAE.\n",
        "\n",
        "    Args:\n",
        "        data_dim (int):\n",
        "            Dimensions of the data.\n",
        "        compress_dims (tuple or list of ints):\n",
        "            Size of each hidden layer.\n",
        "        embedding_dim (int):\n",
        "            Size of the output vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dim, compress_dims, embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        dim = data_dim\n",
        "        seq = []\n",
        "        for item in list(compress_dims):\n",
        "            seq += [Linear(dim, item), ReLU(), Dropout(0.2)]\n",
        "            dim = item\n",
        "\n",
        "        self.seq = Sequential(*seq)\n",
        "        self.fc1 = Linear(dim, embedding_dim)\n",
        "        self.fc2 = Linear(dim, embedding_dim)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        \"\"\"Encode the passed `input_`.\"\"\"\n",
        "        feature = self.seq(input_)\n",
        "        mu = self.fc1(feature)\n",
        "        logvar = self.fc2(feature)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        return mu, std, logvar\n",
        "\n",
        "\n",
        "class Decoder(Module):\n",
        "    \"\"\"Decoder for the TVAE.\n",
        "\n",
        "    Args:\n",
        "        embedding_dim (int):\n",
        "            Size of the input vector.\n",
        "        decompress_dims (tuple or list of ints):\n",
        "            Size of each hidden layer.\n",
        "        data_dim (int):\n",
        "            Dimensions of the data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, decompress_dims, data_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        dim = embedding_dim\n",
        "        seq = []\n",
        "        for item in list(decompress_dims):\n",
        "            seq += [Linear(dim, item), ReLU()]\n",
        "            dim = item\n",
        "\n",
        "        seq.append(Linear(dim, data_dim))\n",
        "        self.seq = Sequential(*seq)\n",
        "        self.sigma = Parameter(torch.ones(data_dim) * 0.1)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        \"\"\"Decode the passed `input_`.\"\"\"\n",
        "        return self.seq(input_), self.sigma\n",
        "\n",
        "\n",
        "def _loss_function(recon_x, x, sigmas, mu, logvar, factor):\n",
        "    loss = []\n",
        "    eq = x - torch.tanh(recon_x)\n",
        "    loss.append((eq**2 / 2 / (sigmas**2)).sum())  #part of the negative log-likelihood\n",
        "    loss.append((torch.log(sigmas) * x.size()[0]).sum())  #part of the negative log-likelihood\n",
        "\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
        "    return sum(loss) * factor / x.size()[0], KLD / x.size()[0]\n",
        "\n",
        "\n",
        "class TVAE():\n",
        "    \"\"\"TVAE.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim=128,\n",
        "        compress_dims=(512, 1024, 512),\n",
        "        decompress_dims=(512, 1024, 512),\n",
        "        l2scale=1e-5,\n",
        "        batch_size=2000,\n",
        "        epochs=800,\n",
        "        loss_factor=1.0,\n",
        "        cuda=True,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.compress_dims = compress_dims\n",
        "        self.decompress_dims = decompress_dims\n",
        "\n",
        "        self.l2scale = l2scale\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_factor = loss_factor\n",
        "        self.epochs = epochs\n",
        "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Batch', 'Loss'])\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if not cuda or not torch.cuda.is_available():\n",
        "            device = 'cpu'\n",
        "        elif isinstance(cuda, str):\n",
        "            device = cuda\n",
        "        else:\n",
        "            device = 'cuda'\n",
        "\n",
        "        self._device = torch.device(device)\n",
        "\n",
        "    #random_state\n",
        "    def fit(self, train_data):\n",
        "\n",
        "        dataset = TensorDataset(torch.from_numpy(train_data.astype('float32')).to(self._device))\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "\n",
        "        data_dim = train_data.shape[1] # embeddings_dim*attributes\n",
        "        encoder = Encoder(data_dim, self.compress_dims, self.embedding_dim).to(self._device)\n",
        "\n",
        "        self.decoder = Decoder(self.embedding_dim, self.decompress_dims, data_dim).to(self._device)\n",
        "        optimizerAE = Adam(\n",
        "            list(encoder.parameters()) + list(self.decoder.parameters()), weight_decay=self.l2scale\n",
        "        )\n",
        "\n",
        "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Batch', 'Loss'])\n",
        "        iterator = tqdm(range(self.epochs), disable=(not self.verbose))\n",
        "        if self.verbose:\n",
        "            iterator_description = 'Loss: {loss:.3f}'\n",
        "            iterator.set_description(iterator_description.format(loss=0))\n",
        "\n",
        "        for i in iterator:\n",
        "            loss_values = []\n",
        "            batch = []\n",
        "            for id_, data in enumerate(loader):\n",
        "                optimizerAE.zero_grad()\n",
        "                real = data[0].to(self._device)\n",
        "                mu, std, logvar = encoder(real)\n",
        "                eps = torch.randn_like(std)\n",
        "                emb = eps * std + mu\n",
        "                rec, sigmas = self.decoder(emb)\n",
        "                loss_1, loss_2 = _loss_function(\n",
        "                    rec,\n",
        "                    real,\n",
        "                    sigmas,\n",
        "                    mu,\n",
        "                    logvar,\n",
        "                    self.loss_factor,\n",
        "                )\n",
        "                loss = loss_1 + loss_2\n",
        "                #print(loss)\n",
        "                loss.backward()\n",
        "                optimizerAE.step()\n",
        "                self.decoder.sigma.data.clamp_(0.01, 1.0)\n",
        "\n",
        "                batch.append(id_)\n",
        "                loss_values.append(loss.detach().cpu().item())\n",
        "\n",
        "            if i%25 == 0:\n",
        "                print(i)\n",
        "\n",
        "            epoch_loss_df = pd.DataFrame({\n",
        "                'Epoch': [i] * len(batch),\n",
        "                'Batch': batch,\n",
        "                'Loss': loss_values,\n",
        "            })\n",
        "            if not self.loss_values.empty:\n",
        "                self.loss_values = pd.concat([self.loss_values, epoch_loss_df]).reset_index(\n",
        "                    drop=True\n",
        "                )\n",
        "            else:\n",
        "                self.loss_values = epoch_loss_df\n",
        "\n",
        "            if self.verbose:\n",
        "                iterator.set_description(\n",
        "                    iterator_description.format(loss=loss.detach().cpu().item())\n",
        "                )\n",
        "\n",
        "    #random_state\n",
        "    def sample(self, samples):\n",
        "\n",
        "        self.decoder.eval()\n",
        "\n",
        "        steps = samples // self.batch_size + 1\n",
        "        data = []\n",
        "        for _ in range(steps):\n",
        "            mean = torch.zeros(self.batch_size, self.embedding_dim)\n",
        "            std = mean + 1\n",
        "            noise = torch.normal(mean=mean, std=std).to(self._device)\n",
        "            fake, sigmas = self.decoder(noise)\n",
        "            fake = torch.tanh(fake)\n",
        "            data.append(fake)\n",
        "\n",
        "        data = torch.cat(data, axis=0)\n",
        "        data = data[:samples]\n",
        "        return data, sigmas\n",
        "\n",
        "    def set_device(self, device):\n",
        "        \"\"\"Set the `device` to be used ('GPU' or 'CPU).\"\"\"\n",
        "        self._device = device\n",
        "        self.decoder.to(self._device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "l_nr = np.log1p(new_rows+0.01)  #For smoothing. It compresses large values and expands small ones\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(l_nr)\n",
        "\n",
        "tvae = TVAE()\n",
        "tvae.fit(X_scaled)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wtk06rCARST5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d,s = tvae.sample(5000)\n",
        "\n",
        "d_tensor = d\n",
        "eps = torch.randn_like(d_tensor)\n",
        "# Sample from N(d, s)\n",
        "sample = d_tensor + torch.sqrt(s) * eps\n",
        "sample = sample.detach().cpu().numpy()\n",
        "\n",
        "sample = scaler.inverse_transform(sample)\n",
        "sample = np.expm1(sample)"
      ],
      "metadata": {
        "id": "FQeljKQSLy9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discrete_values(tmp, start, end):\n",
        "    X = embeddings_df.iloc[:, start : end]\n",
        "    res=[]\n",
        "    score = float('inf')\n",
        "    for x in tmp:\n",
        "        att=None\n",
        "        score = float('inf')\n",
        "        for v in X:\n",
        "            s = nn.CosineEmbeddingLoss()(torch.tensor(X[v].values).unsqueeze(0), torch.tensor(x).unsqueeze(0), torch.ones(x.shape))\n",
        "            if (s<score):\n",
        "               score=s\n",
        "               att=v\n",
        "        res.append(att)\n",
        "    return res\n",
        "\n",
        "\n",
        "def create_final_df(sample, columns, discrete_columns, num_features):\n",
        "    final_df = pd.DataFrame()\n",
        "    i=0\n",
        "    j=0\n",
        "    col_atts_indeces = np.concatenate([np.zeros(1).astype(int), np.cumsum(ebd.values_per_column)])\n",
        "    for c in columns:\n",
        "        if c in discrete_columns:\n",
        "            tmp = sample[:,i:i+num_features]\n",
        "            attributes = get_discrete_values(tmp, col_atts_indeces[j], col_atts_indeces[j+1])\n",
        "            j+=1\n",
        "            final_df[c] = attributes\n",
        "            i+=num_features\n",
        "        else:\n",
        "            final_df[c] = sample[:,i]\n",
        "            i+=1\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "pHVegWspSnIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = create_final_df(sample, new_df.columns, discrete_columns, num_features)"
      ],
      "metadata": {
        "id": "u48RK3VxRPr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "id": "S6JLOdtHSf6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}