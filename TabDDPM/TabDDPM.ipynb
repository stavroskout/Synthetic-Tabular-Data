{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load Drive and Model"
      ],
      "metadata": {
        "id": "hoUqktFyP_eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVGHRrIXbz0m",
        "outputId": "ec6d4513-a0ae-4974-e205-c2c0a4129584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "s5uNB2wTFUHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/TabDDPM')"
      ],
      "metadata": {
        "id": "9wQpvc7ld_8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/TabDDPM')"
      ],
      "metadata": {
        "id": "o6HJsLBPePWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#My dataset"
      ],
      "metadata": {
        "id": "P7u5p7puesmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nameOfDataset=\"...\"\n",
        "\n",
        "!mkdir Results\n",
        "\n",
        "%cd data\n",
        "!mkdir {nameOfDataset}\n",
        "%cd ..\n",
        "%cd exp\n",
        "!mkdir {nameOfDataset}\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHgN3z9FWycP",
        "outputId": "8499d951-3ded-4eed-ee19-605f3483c8f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘Results’: File exists\n",
            "/content/drive/My Drive/TabDDPM/data\n",
            "mkdir: cannot create directory ‘nba’: File exists\n",
            "/content/drive/My Drive/TabDDPM\n",
            "/content/drive/My Drive/TabDDPM/exp\n",
            "mkdir: cannot create directory ‘nba’: File exists\n",
            "/content/drive/My Drive/TabDDPM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nameOfTest=\"...\"\n",
        "\n",
        "%cd Results\n",
        "!mkdir {nameOfTest}\n",
        "%cd {nameOfTest}\n",
        "!mkdir Real\n",
        "%cd ..\n",
        "%cd .."
      ],
      "metadata": {
        "id": "-XwuVpsQXiSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"...\"   # use the name .csv file: nameOfDataset.csv\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(filename):\n",
        "    print(f\"{filename} not found. Please upload it.\")\n",
        "    files.upload()\n",
        "\n",
        "data = pd.read_csv(filename)\n",
        "data = data.dropna().reset_index(drop=True)\n",
        "attributes_to_ignore = ['...']\n",
        "data = data.drop(attributes_to_ignore, axis=1)\n",
        "cat_columns = ['...']\n",
        "data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LKlMcd95XtsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_validate_test_split(df, train_percent=0.99, validate_percent=0.01, seed=1):\n",
        "    np.random.seed(seed)\n",
        "    perm = np.random.permutation(df.index)\n",
        "    m = len(df.index)\n",
        "    train_end = int(train_percent * m)\n",
        "    validate_end = int(validate_percent * m) + train_end\n",
        "    train = df.iloc[perm[:train_end]]\n",
        "    validate = df.iloc[perm[train_end:validate_end]]\n",
        "    test = df.iloc[perm[validate_end:]]\n",
        "    return train, train_end, validate, validate_end-train_end, test, m-validate_end\n",
        "\n",
        "trainingSet, train_size, ValidationSet, val_size, testSet, test_size = train_validate_test_split(data,seed=5)"
      ],
      "metadata": {
        "id": "7M-UzDs6ZN1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pandasDataFrameToNumpy(nameOfDataset, nameOfTest, trainingSet, ValidationSet, testSet):\n",
        "\n",
        "    ##Training Set To Numpy in folder \"Results\\nameOfTest\\Real\\\"\n",
        "    labels= trainingSet.iloc[:,-1]\n",
        "    label_c_name = labels.name\n",
        "    trainingData = trainingSet.drop(trainingSet.columns[-1],axis = 1)\n",
        "    categoricalData = trainingSet[cat_columns]\n",
        "    cat_c_names = list(categoricalData.columns)\n",
        "    numericalData = trainingData.drop(cat_columns,axis = 1)\n",
        "    num_c_names = list(numericalData.columns)\n",
        "    numericalData = numericalData.to_numpy(dtype='float32')\n",
        "\n",
        "\n",
        "    d_dir = os.path.join(\"data\", nameOfDataset)\n",
        "    if not os.path.exists(d_dir):\n",
        "        os.makedirs(d_dir)\n",
        "    r_dir = os.path.join(\"Results\", nameOfTest, \"Real\")\n",
        "    if not os.path.exists(r_dir):\n",
        "        os.makedirs(r_dir)\n",
        "\n",
        "    np.save(os.path.join(d_dir, \"X_num_train.npy\"), numericalData)\n",
        "    np.save(os.path.join(d_dir, \"X_cat_train.npy\"), categoricalData, allow_pickle=True)\n",
        "    np.save(os.path.join(d_dir, \"y_train.npy\"), labels)\n",
        "\n",
        "    np.save(os.path.join(r_dir, \"X_num_train.npy\"), numericalData)\n",
        "    np.save(os.path.join(r_dir, \"X_cat_train.npy\"), categoricalData, allow_pickle=True)\n",
        "    np.save(os.path.join(r_dir, \"y_train.npy\"), labels)\n",
        "\n",
        "    ##Validation Set To Numpy in folder \"Results\\nameOfTest\\Real\\\"\n",
        "    labels= ValidationSet.iloc[:,-1]\n",
        "    validationData = ValidationSet.drop(ValidationSet.columns[-1],axis = 1)\n",
        "    categoricalData = validationData[cat_columns]\n",
        "    numericalData = validationData.drop(cat_columns,axis = 1)\n",
        "    numericalData = numericalData.to_numpy(dtype='float32')\n",
        "\n",
        "    np.save(os.path.join(d_dir, \"X_num_val.npy\"), numericalData)\n",
        "    np.save(os.path.join(d_dir, \"X_cat_val.npy\"), categoricalData, allow_pickle=True)\n",
        "    np.save(os.path.join(d_dir, \"y_val.npy\"), labels)\n",
        "\n",
        "    np.save(os.path.join(r_dir, \"X_num_val.npy\"), numericalData)\n",
        "    np.save(os.path.join(r_dir, \"X_cat_val.npy\"), categoricalData, allow_pickle=True)\n",
        "    np.save(os.path.join(r_dir, \"y_val.npy\"), labels)\n",
        "\n",
        "\n",
        "    ##Test Set To Numpy in folder \"Results\\nameOfTest\\Real\\\"\n",
        "    labels= testSet.iloc[:,-1]\n",
        "    testData = testSet.drop(testSet.columns[-1],axis = 1)\n",
        "    categoricalData = testData[cat_columns]\n",
        "    numericalData = testData.drop(cat_columns,axis = 1)\n",
        "    numericalData = numericalData.to_numpy(dtype='float32')\n",
        "\n",
        "\n",
        "    np.save(os.path.join(d_dir, \"X_num_test.npy\"), numericalData)\n",
        "    np.save(os.path.join(d_dir, \"X_cat_test.npy\"), categoricalData, allow_pickle=True)\n",
        "    np.save(os.path.join(d_dir, \"y_test.npy\"), labels)\n",
        "\n",
        "    np.save(os.path.join(r_dir, \"X_num_test.npy\"), numericalData)\n",
        "    np.save(os.path.join(r_dir, \"X_cat_test.npy\"), categoricalData, allow_pickle=True)\n",
        "    np.save(os.path.join(r_dir, \"y_test.npy\"), labels)\n",
        "\n",
        "    column_names = cat_c_names + num_c_names + [label_c_name]\n",
        "    return column_names"
      ],
      "metadata": {
        "id": "-7c3ol1oZdNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = pandasDataFrameToNumpy(nameOfDataset, nameOfTest, trainingSet, ValidationSet, testSet)"
      ],
      "metadata": {
        "id": "yrALCvZrZk7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names"
      ],
      "metadata": {
        "id": "E_6j-DSTYgBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use your nameOfDataset and nameOfTest and adjust your parameters\n",
        "\n",
        "toml_text = \"\"\"\n",
        "seed = 1\n",
        "parent_dir = \"exp/{nameOfDataset}/{nameOfTest}\"\n",
        "real_data_path = \"data/{nameOfDataset}/\"\n",
        "model_type = \"mlp\"\n",
        "num_numerical_features = ...\n",
        "device = \"cuda:0\"\n",
        "\n",
        "[model_params]\n",
        "num_classes=0\n",
        "is_y_cond = false\n",
        "\n",
        "[model_params.rtdl_params]\n",
        "d_layers = [\n",
        "    512,\n",
        "    1028,\n",
        "    256,\n",
        "]\n",
        "dropout = 0.0\n",
        "\n",
        "[diffusion_params]\n",
        "num_timesteps = 1000\n",
        "gaussian_loss_type = \"mse\"\n",
        "scheduler = \"cosine\"\n",
        "\n",
        "[train.main]\n",
        "steps = 8000\n",
        "lr = 0.001\n",
        "weight_decay = 1e-05\n",
        "batch_size = 100\n",
        "\n",
        "[train.T]\n",
        "seed = 0\n",
        "normalization = \"quantile\"\n",
        "num_nan_policy = \"__none__\"\n",
        "cat_nan_policy = \"__none__\"\n",
        "cat_min_frequency = \"__none__\"\n",
        "cat_encoding = \"__none__\"\n",
        "y_policy = \"default\"\n",
        "\n",
        "[sample]\n",
        "num_samples = 300\n",
        "batch_size = 100\n",
        "seed = 0\n",
        "\n",
        "[eval.type]\n",
        "eval_model = \"catboost\"\n",
        "eval_type = \"synthetic\"\n",
        "\n",
        "[eval.T]\n",
        "seed = 0\n",
        "normalization = \"__none__\"\n",
        "num_nan_policy = \"__none__\"\n",
        "cat_nan_policy = \"__none__\"\n",
        "cat_min_frequency = \"__none__\"\n",
        "cat_encoding = \"__none__\"\n",
        "y_policy = \"default\"\n",
        "\"\"\"\n",
        "\n",
        "toml_dir = os.path.join(\"exp\", nameOfDataset, nameOfTest)\n",
        "os.makedirs(toml_dir, exist_ok=True)\n",
        "with open(os.path.join(toml_dir, \"config.toml\"), \"w\") as f:\n",
        "    f.write(toml_text)"
      ],
      "metadata": {
        "id": "DNcRngnp_0PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_size)\n",
        "print(val_size)\n",
        "print(test_size)"
      ],
      "metadata": {
        "id": "yqYAUOEwFMXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info_txt=\"\"\"\n",
        "{\n",
        "    \"name\": \"...\",\n",
        "    \"id\": \"...\",\n",
        "    \"task_type\": \"regression\",\n",
        "    \"n_num_features\": ...,\n",
        "    \"n_cat_features\": ...,\n",
        "    \"train_size\": ...,\n",
        "    \"val_size\": ...,\n",
        "    \"test_size\": ...\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "info_dir = os.path.join(\"data\", nameOfDataset)\n",
        "with open(os.path.join(info_dir, \"info.json\"), \"w\") as f:\n",
        "    f.write(info_txt)"
      ],
      "metadata": {
        "id": "ltUgRbPQD9uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "metadata": {
        "id": "wDR3wSzcd3Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost category-encoders dython icecream libzero optuna pyarrow rtdl scipy skorch tomli-w tomli tqdm"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEhm243tWm0r",
        "outputId": "630f5454-1e96-4afe-d7ce-221c9e496992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting category-encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting dython\n",
            "  Downloading dython-0.7.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting icecream\n",
            "  Downloading icecream-2.1.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting libzero\n",
            "  Downloading libzero-0.0.8-py3-none-any.whl.metadata (888 bytes)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting rtdl\n",
            "  Downloading rtdl-0.0.13-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Collecting skorch\n",
            "  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tomli-w\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tomli\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (1.6.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (0.14.4)\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from dython) (0.13.2)\n",
            "Requirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.11/dist-packages (from dython) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dython) (75.2.0)\n",
            "Collecting colorama>=0.3.9 (from icecream)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from icecream) (2.19.1)\n",
            "Collecting executing>=2.1.0 (from icecream)\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting numpy<3.0,>=1.16.0 (from catboost)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynvml<12,>=11.0 (from libzero)\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting torch<2,>=1.7 (from libzero)\n",
            "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category-encoders) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category-encoders) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2,>=1.7->libzero)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2,>=1.7->libzero)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch<2,>=1.7->libzero)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2,>=1.7->libzero)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7->libzero) (0.45.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dython-0.7.9-py3-none-any.whl (26 kB)\n",
            "Downloading icecream-2.1.4-py3-none-any.whl (14 kB)\n",
            "Downloading libzero-0.0.8-py3-none-any.whl (28 kB)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtdl-0.0.13-py3-none-any.whl (23 kB)\n",
            "Downloading skorch-1.1.0-py3-none-any.whl (228 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: tomli-w, tomli, pynvml, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, executing, colorlog, colorama, asttokens, nvidia-cudnn-cu11, icecream, alembic, torch, optuna, skorch, rtdl, libzero, category-encoders, catboost, dython\n",
            "  Attempting uninstall: pynvml\n",
            "    Found existing installation: pynvml 12.0.0\n",
            "    Uninstalling pynvml-12.0.0:\n",
            "      Successfully uninstalled pynvml-12.0.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask-cuda 25.2.0 requires pynvml<13.0.0a0,>=12.0.0, but you have pynvml 11.5.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "ucx-py-cu12 0.42.0 requires pynvml<13.0.0a0,>=12.0.0, but you have pynvml 11.5.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pynvml<13.0.0a0,>=12.0.0, but you have pynvml 11.5.3 which is incompatible.\n",
            "torchdata 0.11.0 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
            "accelerate 1.7.0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "ucxx-cu12 0.42.0 requires pynvml<13.0.0a0,>=12.0.0, but you have pynvml 11.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alembic-1.16.1 asttokens-3.0.0 catboost-1.2.8 category-encoders-2.8.1 colorama-0.4.6 colorlog-6.9.0 dython-0.7.9 executing-2.2.0 icecream-2.1.4 libzero-0.0.8 numpy-1.26.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 optuna-4.3.0 pynvml-11.5.3 rtdl-0.0.13 skorch-1.1.0 tomli-2.2.1 tomli-w-1.2.0 torch-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m scripts.pipeline --config exp/nba/nbaTest/config.toml --train --sample"
      ],
      "metadata": {
        "id": "VrVMje75Zuvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_file_path =  '/content/drive/MyDrive/TabDDPM/exp/nba/nbaTest/X_num_train.npy'\n",
        "X_num_gen = np.load(num_file_path, allow_pickle=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wEm478mtXxAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_file_path =  '/content/drive/MyDrive/TabDDPM/exp/nba/nbaTest/X_cat_train.csv'\n",
        "df_loaded = pd.read_csv(cat_file_path)\n",
        "X_cat_gen = df_loaded.values"
      ],
      "metadata": {
        "id": "zf9n9fFX2V8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_file_path = '/content/drive/MyDrive/TabDDPM/exp/nba/nbaTest/y_train.npy'\n",
        "y_gen = np.load(y_file_path, allow_pickle=True)"
      ],
      "metadata": {
        "id": "caTjo1ha8wB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if y_gen.shape[1:] == ():\n",
        "    y_gen = y_gen.reshape(-1, 1)\n",
        "if X_cat_gen.shape[1:] == ():\n",
        "    X_cat_gen = X_cat_gen.reshape(-1, 1)\n",
        "if X_num_gen.shape[1:] == ():\n",
        "    X_num_gen = X_num_gen.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "nFb1q__TeU8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.DataFrame(np.concatenate([X_cat_gen, X_num_gen, y_gen], axis=1), columns=column_names)\n",
        "final_df"
      ],
      "metadata": {
        "id": "6AInWv1p8zai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}